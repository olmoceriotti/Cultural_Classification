{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wX38ebuVIV_v"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8gGs8aQLIR6S"
   },
   "outputs": [],
   "source": [
    "!pip install datasets scikit-learn pandas numpy spacy xgboost wikidata -q\n",
    "!python -m spacy download en_core_web_sm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KCe2qLolIbP4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import spacy\n",
    "import re\n",
    "from collections import Counter\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import os\n",
    "import pickle\n",
    "from google.colab import drive\n",
    "from sklearn.metrics import confusion_matrix\n",
    "try:\n",
    "    from wikidata.client import Client\n",
    "    from wikidata.entity import EntityId\n",
    "    print(\"Wikidata library imported successfully.\")\n",
    "except ImportError:\n",
    "    print(\"Wikidata library not found. Please ensure 'pip install Wikidata' succeeded.\")\n",
    "    Client = None\n",
    "\n",
    "from datasets import load_dataset\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import classification_report, make_scorer, f1_score, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.sparse import hstack\n",
    "from scipy.stats import randint, uniform\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r-tP_WoBId6m"
   },
   "outputs": [],
   "source": [
    "drive.mount('/content/drive', force_remount=True)\n",
    "base_drive_path = '/content/drive/MyDrive/Bert&Ernie_shared_folder/'\n",
    "data_path = os.path.join(base_drive_path, 'data')\n",
    "models_path = os.path.join(base_drive_path, 'models')\n",
    "results_path = os.path.join(base_drive_path, 'results')\n",
    "plot_save_dir = os.path.join(results_path, 'plots_no_transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "exsTwKfzIg2A"
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='sklearn')\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module='xgboost')\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\") # This model is NOT transformer based\n",
    "    print(\"spaCy model loaded successfully.\")\n",
    "except OSError:\n",
    "    print('Downloading spaCy model...')\n",
    "    from spacy.cli import download\n",
    "    download(\"en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"spaCy model downloaded and loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xhClhbTlU_kD"
   },
   "outputs": [],
   "source": [
    "!huggingface-cli login --token #INSERT TOKEN HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8eTzqy_Iini"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RuhIVdqYI0pR"
   },
   "outputs": [],
   "source": [
    "print(\"Loading dataset...\")\n",
    "dataset = load_dataset('sapienzanlp/nlp2025_hw1_cultural_dataset')\n",
    "train_data = dataset[\"train\"]\n",
    "val_data = dataset[\"validation\"]\n",
    "train_df = pd.DataFrame(train_data)\n",
    "val_df = pd.DataFrame(val_data)\n",
    "print(f\"Train data shape: {train_df.shape}\")\n",
    "print(f\"Validation data shape: {val_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pgdr2OAlJAq8"
   },
   "outputs": [],
   "source": [
    "def combine_text(row):\n",
    "    parts = [str(row.get(col, '')) for col in ['name', 'description', 'type', 'category', 'subcategory']]\n",
    "    return \" \".join(filter(None, parts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_GQkyhVJDoG"
   },
   "outputs": [],
   "source": [
    "print(\"Combining text fields...\")\n",
    "train_df['combined_text'] = train_df.apply(combine_text, axis=1)\n",
    "val_df['combined_text'] = val_df.apply(combine_text, axis=1)\n",
    "\n",
    "label_map = {'cultural agnostic': 0, 'cultural exclusive': 1, 'cultural representative': 2}\n",
    "inverse_label_map = {v: k for k, v in label_map.items()}\n",
    "target_names = list(label_map.keys())\n",
    "\n",
    "train_df['label_id'] = train_df['label'].map(label_map)\n",
    "val_df['label_id'] = val_df['label'].map(label_map)\n",
    "\n",
    "y_train = train_df['label_id'].values\n",
    "y_val = val_df['label_id'].values\n",
    "\n",
    "print(\"Data preprocessing complete.\")\n",
    "wikidata_client = Client() if Client else None\n",
    "wikidata_cache = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JG08Uf0tOQP6"
   },
   "source": [
    "### Wikidata query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qQItc_U2JHK3"
   },
   "outputs": [],
   "source": [
    "def get_qid_from_url(url):\n",
    "    if isinstance(url, str) and 'wikidata.org/entity/Q' in url:\n",
    "        return url.split('/')[-1]\n",
    "    return None\n",
    "\n",
    "def parse_wikidata_date(date_str):\n",
    "    if not date_str or not isinstance(date_str, str):\n",
    "        return None\n",
    "    match = re.match(r'\\+?(-?\\d{4})', date_str)\n",
    "    if match:\n",
    "        try:\n",
    "            year = int(match.group(1))\n",
    "            if year > -4000 and year != 0:\n",
    "                 return year\n",
    "        except ValueError:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def get_wikidata_features(qid):\n",
    "    if not qid or not wikidata_client:\n",
    "        return {'sitelink_count': 0, 'has_country_of_origin': 0, 'item_age': np.nan}\n",
    "\n",
    "    if qid in wikidata_cache:\n",
    "        return wikidata_cache[qid]\n",
    "\n",
    "    sitelinks = 0\n",
    "    has_origin = 0\n",
    "    inception_year = None\n",
    "    item_age = np.nan\n",
    "    current_year = datetime.now().year\n",
    "\n",
    "    try:\n",
    "        entity_data = wikidata_client.get(EntityId(qid), load=True).data\n",
    "\n",
    "        if 'sitelinks' in entity_data and isinstance(entity_data['sitelinks'], dict):\n",
    "            sitelinks = len(entity_data['sitelinks'])\n",
    "\n",
    "        P495_qid = 'P495'\n",
    "        P571_qid = 'P571'\n",
    "        P577_qid = 'P577'\n",
    "\n",
    "        if P495_qid in entity_data.get('claims', {}):\n",
    "            if len(entity_data['claims'][P495_qid]) > 0:\n",
    "                 has_origin = 1\n",
    "\n",
    "        date_prop_qid = None\n",
    "        if P571_qid in entity_data.get('claims', {}):\n",
    "            date_prop_qid = P571_qid\n",
    "        elif P577_qid in entity_data.get('claims', {}):\n",
    "             date_prop_qid = P577_qid\n",
    "\n",
    "        if date_prop_qid:\n",
    "             try:\n",
    "                 claim_list = entity_data.get('claims', {}).get(date_prop_qid, [])\n",
    "                 if claim_list:\n",
    "                    datavalue = claim_list[0].get('mainsnak', {}).get('datavalue', {})\n",
    "                    if datavalue and datavalue.get('type') == 'time':\n",
    "                        time_string = datavalue.get('value', {}).get('time')\n",
    "                        if time_string:\n",
    "                            inception_year = parse_wikidata_date(time_string)\n",
    "\n",
    "             except (KeyError, IndexError, TypeError, AttributeError) as date_e:\n",
    "                 inception_year = None\n",
    "\n",
    "        if inception_year is not None:\n",
    "            if inception_year <= current_year:\n",
    "                 item_age = current_year - inception_year\n",
    "            else:\n",
    "                 item_age = 0\n",
    "\n",
    "    except Exception as e:\n",
    "        return {'sitelink_count': 0, 'has_country_of_origin': 0, 'item_age': np.nan}\n",
    "\n",
    "    result = {'sitelink_count': sitelinks, 'has_country_of_origin': has_origin, 'item_age': item_age}\n",
    "    wikidata_cache[qid] = result\n",
    "    time.sleep(0.05)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7aTL1WgOWFs"
   },
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jp3DlsJuJW6j"
   },
   "outputs": [],
   "source": [
    "nationality_keywords = [\n",
    "    'italian', 'italy', 'roman', 'rome', 'florence', 'venice', 'sicilian', 'neapolitan', 'lazio',\n",
    "    'french', 'france', 'paris', 'provence', 'gaulish', 'norman',\n",
    "    'german', 'germany', 'bavarian', 'prussian', 'berlin',\n",
    "    'spanish', 'spain', 'catalan', 'andalusian', 'castilian', 'madrid', 'barcelona',\n",
    "    'british', 'britain', 'uk', 'english', 'scottish', 'welsh', 'irish', 'london', 'celtic',\n",
    "    'greek', 'greece', 'hellenic', 'athenian', 'spartan', 'crete', 'byzantine',\n",
    "    'russian', 'russia', 'slavic', 'moscow', 'soviet',\n",
    "    'portuguese', 'portugal', 'lisbon',\n",
    "    'dutch', 'netherlands', 'amsterdam',\n",
    "    'belgian', 'belgium',\n",
    "    'swiss', 'switzerland',\n",
    "    'austrian', 'austria', 'viennese',\n",
    "    'swedish', 'sweden', 'norwegian', 'norway', 'finnish', 'finland', 'scandinavian', 'viking',\n",
    "    'polish', 'poland', 'hungarian', 'hungary', 'czech',\n",
    "    'european',\n",
    "    'chinese', 'china', 'mandarin', 'cantonese', 'beijing', 'shanghai', 'han', 'tang', 'ming', 'qing',\n",
    "    'japanese', 'japan', 'tokyo', 'kyoto', 'edo', 'samurai', 'shinto', 'zen',\n",
    "    'korean', 'korea', 'seoul',\n",
    "    'indian', 'india', 'hindi', 'sanskrit', 'mughal', 'vedic', 'hindu', 'buddhist', 'delhi', 'mumbai', 'bengali',\n",
    "    'thai', 'thailand', 'vietnamese', 'vietnam',\n",
    "    'indonesian', 'indonesia', 'malaysian', 'malaysia',\n",
    "    'filipino', 'philippines',\n",
    "    'turkish', 'turkey', 'ottoman', 'istanbul',\n",
    "    'persian', 'iran', 'iranian', 'farsi',\n",
    "    'arab', 'arabic', 'arabian',\n",
    "    'israeli', 'israel', 'hebrew', 'jewish',\n",
    "    'asian',\n",
    "    'american', 'usa', 'us', 'new york', 'hollywood',\n",
    "    'canadian', 'canada',\n",
    "    'mexican', 'mexico', 'aztec', 'mayan', 'nahuatl',\n",
    "    'cuban', 'cuba',\n",
    "    'brazilian', 'brazil', 'rio',\n",
    "    'argentinian', 'argentine', 'buenos aires',\n",
    "    'peruvian', 'peru', 'inca', 'quechua',\n",
    "    'colombian', 'colombia',\n",
    "    'native american', 'indigenous american',\n",
    "    'egyptian', 'egypt', 'cairo', 'pharaoh', 'ancient egyptian',\n",
    "    'moroccan', 'morocco',\n",
    "    'ethiopian', 'ethiopia',\n",
    "    'nigerian', 'nigeria', 'yoruba', 'igbo',\n",
    "    'kenyan', 'kenya',\n",
    "    'south african', 'south africa', 'zulu',\n",
    "    'african',\n",
    "    'australian', 'australia', 'aboriginal australian',\n",
    "    'new zealander', 'new zealand', 'maori',\n",
    "    'polynesian', 'hawaiian', 'samoan', 'tongan',\n",
    "    'islamic', 'muslim', 'christian', 'catholic', 'protestant', 'orthodox', 'jewish', 'judaism', 'buddhist', 'hindu',\n",
    "    'gypsy', 'roma',\n",
    "    'latin',\n",
    "]\n",
    "\n",
    "cultural_keywords = [\n",
    "    'traditional', 'tradition', 'custom', 'customary', 'ritual', 'rite', 'ceremony', 'ceremonial',\n",
    "    'heritage', 'historical', 'ancient', 'classical', 'medieval', 'renaissance',\n",
    "    'folk', 'folklore', 'myth', 'legend', 'mythology', 'sacred', 'holy', 'religious', 'spiritual', 'belief',\n",
    "    'indigenous', 'native', 'vernacular', 'dialect',\n",
    "    'regional', 'local', 'provincial', 'ethnic', 'ethnicity',\n",
    "    'artisan', 'handcrafted', 'handmade', 'guild',\n",
    "    'cuisine', 'recipe', 'gastronomy',\n",
    "    'art', 'music', 'dance', 'theatre', 'literature', 'architecture', 'philosophy',\n",
    "    'style', 'genre', 'form', 'technique',\n",
    "    'unique', 'distinctive', 'specific to', 'characteristic', 'endemic',\n",
    "    'social', 'societal', 'community', 'tribe', 'clan', 'caste', 'dynasty', 'kingdom', 'empire',\n",
    "    'symbol', 'symbolic', 'iconic'\n",
    "]\n",
    "\n",
    "global_keywords = [\n",
    "    'global', 'worldwide', 'world', 'international', 'universal', 'ubiquitous', 'widespread', 'transnational',\n",
    "    'common', 'standard', 'standardized', 'basic', 'fundamental', 'essential', 'general', 'generic',\n",
    "    'modern', 'contemporary', 'current', 'recent', 'new',\n",
    "    'popular', 'famous', 'well-known',\n",
    "    'scientific', 'science', 'technological', 'technology', 'digital', 'electronic', 'mechanical', 'engineering',\n",
    "    'mathematical', 'mathematics', 'physics', 'chemistry', 'biology', 'medical', 'computational',\n",
    "    'concept', 'idea', 'theory', 'principle', 'method', 'system', 'structure', 'process', 'framework', 'model',\n",
    "    'tool', 'instrument', 'device', 'machine', 'vehicle',\n",
    "    'data', 'information', 'analysis', 'measurement',\n",
    "    'human', 'person', 'people'\n",
    "]\n",
    "\n",
    "representative_keywords = [\n",
    "    'popularized', 'adapted', 'adaptation', 'variation', 'variant', 'fusion', 'hybrid',\n",
    "    'influenced', 'inspired', 'derived',\n",
    "    'spread', 'exported', 'imported', 'adopted', 'introduced',\n",
    "    'version', 'interpretation', 'style of'\n",
    "]\n",
    "\n",
    "\n",
    "def extract_ner_lexicon_features(text):\n",
    "    features = Counter()\n",
    "    doc = nlp(text)\n",
    "\n",
    "    target_ner_labels = ['GPE', 'LOC', 'NORP', 'ORG', 'FAC', 'EVENT', 'LANGUAGE', 'PERSON', 'WORK_OF_ART', 'PRODUCT']\n",
    "    ner_entity_count = 0\n",
    "    ner_nationality_group_count = 0\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in target_ner_labels:\n",
    "            features[f'ner_{ent.label_}_count'] += 1\n",
    "            ner_entity_count += 1\n",
    "            if ent.label_ in ['GPE', 'NORP', 'LOC']:\n",
    "                 ner_nationality_group_count += 1\n",
    "\n",
    "    features['ner_total_entities'] = ner_entity_count\n",
    "    features['ner_total_geo_nat_entities'] = ner_nationality_group_count\n",
    "\n",
    "    text_lower = text.lower()\n",
    "    counts = {'cultural': 0, 'nationality': 0, 'global': 0, 'representative': 0}\n",
    "    keyword_lists = {\n",
    "        'cultural': cultural_keywords,\n",
    "        'nationality': nationality_keywords,\n",
    "        'global': global_keywords,\n",
    "        'representative': representative_keywords\n",
    "    }\n",
    "\n",
    "    for category, keywords in keyword_lists.items():\n",
    "        for keyword in keywords:\n",
    "             if re.search(r'\\b' + re.escape(keyword) + r'\\b', text_lower):\n",
    "                 features[f'lex_{category}_keyword_{keyword}'] = 1\n",
    "                 counts[category] += 1\n",
    "\n",
    "    features['lex_total_cultural'] = counts['cultural']\n",
    "    features['lex_total_nationality'] = counts['nationality']\n",
    "    features['lex_total_global'] = counts['global']\n",
    "    features['lex_total_representative'] = counts['representative']\n",
    "\n",
    "    features['text_length_chars'] = len(text)\n",
    "    features['text_length_tokens'] = len(doc)\n",
    "\n",
    "    total_tokens = len(doc) if len(doc) > 0 else 1\n",
    "    features['ratio_cultural_keywords'] = counts['cultural'] / total_tokens\n",
    "    features['ratio_nationality_keywords'] = counts['nationality'] / total_tokens\n",
    "    features['ratio_global_keywords'] = counts['global'] / total_tokens\n",
    "    features['ratio_representative_keywords'] = counts['representative'] / total_tokens\n",
    "    features['ratio_ner_entities'] = ner_entity_count / total_tokens\n",
    "    features['ratio_ner_geo_nat_entities'] = ner_nationality_group_count / total_tokens\n",
    "\n",
    "    features['diff_nationality_global'] = counts['nationality'] - counts['global']\n",
    "    features['diff_cultural_global'] = counts['cultural'] - counts['global']\n",
    "    features['ratio_nationality_vs_global'] = counts['nationality'] / (counts['global'] + 1e-6)\n",
    "    features['ratio_cultural_vs_global'] = counts['cultural'] / (counts['global'] + 1e-6)\n",
    "\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QzuWjyJoJbEi"
   },
   "outputs": [],
   "source": [
    "CACHE_FILE = os.path.join(data_path, 'wikidata_feature_cache.pkl')\n",
    "\n",
    "try:\n",
    "    if os.path.exists(CACHE_FILE):\n",
    "        with open(CACHE_FILE, 'rb') as f:\n",
    "            wikidata_cache = pickle.load(f)\n",
    "        print(f\"Loaded Wikidata feature cache from {CACHE_FILE}, {len(wikidata_cache)} entries.\")\n",
    "    else:\n",
    "        wikidata_cache = {}\n",
    "        print(f\"No existing cache found at {CACHE_FILE}. Starting fresh.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading cache from {CACHE_FILE}: {e}. Using in-memory cache only for this session.\")\n",
    "    CACHE_FILE = None\n",
    "    wikidata_cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uwid5XfNJiz-"
   },
   "outputs": [],
   "source": [
    "print(\"\\nProcessing Wikidata features (fetching missing, using cache)...\")\n",
    "wikidata_fetch_errors = 0\n",
    "wikidata_cache_hits = 0\n",
    "wikidata_newly_fetched = 0\n",
    "\n",
    "if 'qid' not in train_df.columns:\n",
    "     train_df['qid'] = train_df['item'].apply(get_qid_from_url)\n",
    "if 'qid' not in val_df.columns:\n",
    "     val_df['qid'] = val_df['item'].apply(get_qid_from_url)\n",
    "\n",
    "def apply_wikidata_features(qid_series):\n",
    "    global wikidata_cache_hits, wikidata_newly_fetched, wikidata_fetch_errors\n",
    "    processed_features = []\n",
    "    qids_to_fetch = []\n",
    "\n",
    "    for qid in qid_series:\n",
    "        if qid and qid in wikidata_cache:\n",
    "            processed_features.append(wikidata_cache[qid])\n",
    "            wikidata_cache_hits += 1\n",
    "        elif qid:\n",
    "             qids_to_fetch.append(qid)\n",
    "             processed_features.append(None)\n",
    "        else:\n",
    "             processed_features.append({'sitelink_count': 0, 'has_country_of_origin': 0, 'item_age': np.nan})\n",
    "\n",
    "    print(f\"Identified {len(qids_to_fetch)} unique QIDs needing fetch...\")\n",
    "    if qids_to_fetch and wikidata_client:\n",
    "        unique_qids_to_fetch = list(set(qids_to_fetch))\n",
    "        print(f\"Fetching data for {len(unique_qids_to_fetch)} unique QIDs...\")\n",
    "        start_fetch_time = time.time()\n",
    "        for qid_to_fetch in unique_qids_to_fetch:\n",
    "             fetched_data = get_wikidata_features(qid_to_fetch)\n",
    "             if fetched_data['sitelink_count']==0 and fetched_data['has_country_of_origin']==0 and np.isnan(fetched_data['item_age']):\n",
    "                 wikidata_fetch_errors +=1\n",
    "             wikidata_newly_fetched += 1\n",
    "        print(f\"Fetching took {time.time() - start_fetch_time:.2f}s\")\n",
    "\n",
    "\n",
    "    final_features_list = []\n",
    "    for i, qid in enumerate(qid_series):\n",
    "        if processed_features[i] is not None:\n",
    "            final_features_list.append(processed_features[i])\n",
    "        elif qid and qid in wikidata_cache:\n",
    "             final_features_list.append(wikidata_cache[qid])\n",
    "        else:\n",
    "             final_features_list.append({'sitelink_count': 0, 'has_country_of_origin': 0, 'item_age': np.nan})\n",
    "\n",
    "    return final_features_list\n",
    "\n",
    "\n",
    "train_wiki_list = apply_wikidata_features(train_df['qid'])\n",
    "val_wiki_list = apply_wikidata_features(val_df['qid'])\n",
    "\n",
    "print(f\"Wikidata processing summary: Cache Hits={wikidata_cache_hits}, Newly Fetched={wikidata_newly_fetched}, Fetch Errors={wikidata_fetch_errors}\")\n",
    "\n",
    "if CACHE_FILE:\n",
    "    try:\n",
    "        with open(CACHE_FILE, 'wb') as f:\n",
    "            pickle.dump(wikidata_cache, f)\n",
    "        print(f\"Wikidata cache saved/updated to {CACHE_FILE}, now {len(wikidata_cache)} entries.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving cache to {CACHE_FILE}: {e}\")\n",
    "\n",
    "\n",
    "train_wiki_df = pd.json_normalize(train_wiki_list).add_prefix('wd_')\n",
    "val_wiki_df = pd.json_normalize(val_wiki_list).add_prefix('wd_')\n",
    "\n",
    "train_df = pd.concat([train_df.drop(columns=[c for c in train_wiki_df.columns if c in train_df.columns], errors='ignore'), train_wiki_df], axis=1)\n",
    "val_df = pd.concat([val_df.drop(columns=[c for c in val_wiki_df.columns if c in val_df.columns], errors='ignore'), val_wiki_df], axis=1)\n",
    "\n",
    "print(\"Wikidata features merged into DataFrames.\")\n",
    "print(\"Sample Train DF columns:\", train_df.columns.tolist())\n",
    "\n",
    "required_wd_cols = ['wd_sitelink_count', 'wd_has_country_of_origin', 'wd_item_age']\n",
    "for df in [train_df, val_df]:\n",
    "    for col in required_wd_cols:\n",
    "        if col not in df.columns:\n",
    "             print(f\"Warning: Column '{col}' missing after load/fetch. Adding placeholder.\")\n",
    "             df[col] = 0 if 'count' in col or 'has' in col else np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XbGxU9NMJkYO"
   },
   "outputs": [],
   "source": [
    "print(\"\\nPreparing Feature Set 1: TF-IDF Baseline...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), min_df=3)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_df['combined_text'])\n",
    "X_val_tfidf = tfidf_vectorizer.transform(val_df['combined_text'])\n",
    "print(f\"TF-IDF Matrix Shape (Train): {X_train_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JraOeDBpJmEl"
   },
   "outputs": [],
   "source": [
    "print(\"\\nPreparing Feature Set 2: NER + Lexicon...\")\n",
    "start_time = time.time()\n",
    "train_features_list_nerlex = train_df['combined_text'].apply(extract_ner_lexicon_features).tolist()\n",
    "val_features_list_nerlex = val_df['combined_text'].apply(extract_ner_lexicon_features).tolist()\n",
    "print(f\"NER/Lexicon feature extraction took {time.time() - start_time:.2f}s\")\n",
    "\n",
    "vectorizer_nerlex = DictVectorizer(sparse=True)\n",
    "X_train_nerlex = vectorizer_nerlex.fit_transform(train_features_list_nerlex)\n",
    "X_val_nerlex = vectorizer_nerlex.transform(val_features_list_nerlex)\n",
    "print(f\"NER+Lexicon Matrix Shape (Train): {X_train_nerlex.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LjXibOQgJnSh"
   },
   "outputs": [],
   "source": [
    "print(\"\\nPreparing Feature Set 3: Hybrid Features...\")\n",
    "\n",
    "wikidata_numerical_cols = ['wd_sitelink_count', 'wd_item_age']\n",
    "wikidata_binary_cols = ['wd_has_country_of_origin']\n",
    "\n",
    "imputer_numerical = SimpleImputer(strategy='median')\n",
    "train_df[wikidata_numerical_cols] = imputer_numerical.fit_transform(train_df[wikidata_numerical_cols])\n",
    "val_df[wikidata_numerical_cols] = imputer_numerical.transform(val_df[wikidata_numerical_cols])\n",
    "\n",
    "imputer_binary = SimpleImputer(strategy='constant', fill_value=0)\n",
    "train_df[wikidata_binary_cols] = imputer_binary.fit_transform(train_df[wikidata_binary_cols])\n",
    "val_df[wikidata_binary_cols] = imputer_binary.transform(val_df[wikidata_binary_cols])\n",
    "\n",
    "scaler_wd = StandardScaler()\n",
    "X_train_wd_numeric_scaled = scaler_wd.fit_transform(train_df[wikidata_numerical_cols])\n",
    "X_val_wd_numeric_scaled = scaler_wd.transform(val_df[wikidata_numerical_cols])\n",
    "\n",
    "X_train_wd_binary = train_df[wikidata_binary_cols].values\n",
    "X_val_wd_binary = val_df[wikidata_binary_cols].values\n",
    "\n",
    "X_train_hybrid = hstack([\n",
    "    X_train_tfidf,\n",
    "    X_train_nerlex,\n",
    "    X_train_wd_numeric_scaled,\n",
    "    X_train_wd_binary\n",
    "], format='csr')\n",
    "\n",
    "X_val_hybrid = hstack([\n",
    "    X_val_tfidf,\n",
    "    X_val_nerlex,\n",
    "    X_val_wd_numeric_scaled,\n",
    "    X_val_wd_binary\n",
    "], format='csr')\n",
    "\n",
    "print(f\"Hybrid (TFIDF+NERLEX+WD) Matrix Shape (Train): {X_train_hybrid.shape}\")\n",
    "\n",
    "\n",
    "f1_macro_scorer = make_scorer(f1_score, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "229g6nr-Ohpm"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8uMfArbQLz4r"
   },
   "outputs": [],
   "source": [
    "def train_evaluate_model(model, params, X_train_feat, y_train_labels, X_val_feat, y_val_labels, use_random_search=False, n_iter=20):\n",
    "    model_name = model.__class__.__name__\n",
    "    print(f\"\\n--- Training and Tuning {model_name} ---\")\n",
    "    if params:\n",
    "        print(\"Starting hyperparameter search...\")\n",
    "        start_time = time.time()\n",
    "        if use_random_search:\n",
    "            search = RandomizedSearchCV(model, params, n_iter=n_iter, cv=5, scoring=f1_macro_scorer,verbose=1, n_jobs=1, random_state=42)\n",
    "        else:\n",
    "            search = GridSearchCV(model, params, cv=5, scoring=f1_macro_scorer,verbose=1, n_jobs=1)\n",
    "\n",
    "        try:\n",
    "            fit_params = {}\n",
    "            if isinstance(model, xgb.XGBClassifier):\n",
    "                fit_params['eval_set'] = [(X_val_feat, y_val_labels)]\n",
    "\n",
    "            search.fit(X_train_feat, y_train_labels, **fit_params)\n",
    "            print(f\"Tuning completed in {time.time() - start_time:.2f} seconds\")\n",
    "            print(f\"Best parameters: {search.best_params_}\")\n",
    "            print(f\"Best cross-validation F1-macro score: {search.best_score_:.4f}\")\n",
    "            best_model = search.best_estimator_\n",
    "        except Exception as e:\n",
    "             print(f\"ERROR during {model_name} tuning: {e}\")\n",
    "             print(\"Falling back to default parameters.\")\n",
    "             fit_params = {}\n",
    "             if isinstance(model, xgb.XGBClassifier):\n",
    "                 fit_params['eval_set'] = [(X_val_feat, y_val_labels)]\n",
    "             model.fit(X_train_feat, y_train_labels, **fit_params)\n",
    "             best_model = model\n",
    "    else:\n",
    "        print(\"Training with default parameters...\")\n",
    "        start_time = time.time()\n",
    "        model.fit(X_train_feat, y_train_labels)\n",
    "        print(f\"Training completed in {time.time() - start_time:.2f} seconds\")\n",
    "        best_model = model\n",
    "    print(f\"\\nValidation Performance for {model_name}:\")\n",
    "    try:\n",
    "        y_pred = best_model.predict(X_val_feat)\n",
    "        report = classification_report(y_val_labels, y_pred, target_names=target_names, zero_division=0)\n",
    "        print(report)\n",
    "        f1_val = f1_score(y_val_labels, y_pred, average='macro')\n",
    "        acc_val = accuracy_score(y_val_labels, y_pred)\n",
    "        print(f\"Validation F1-macro: {f1_val:.4f}\")\n",
    "        print(f\"Validation Accuracy: {acc_val:.4f}\")\n",
    "        return best_model, f1_val\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR during prediction/evaluation for {model_name}: {e}\")\n",
    "        return model, 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FYHBaLgbL8V4"
   },
   "outputs": [],
   "source": [
    "feature_sets = {\n",
    "    \"TF-IDF\": (X_train_tfidf, X_val_tfidf),\n",
    "    \"NER+Lexicon\": (X_train_nerlex, X_val_nerlex),\n",
    "    \"Hybrid (TFIDF+NERLEX+WD)\": (X_train_hybrid, X_val_hybrid)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "models_to_try = {\n",
    "    \"LogisticRegression\": (\n",
    "        LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced', solver='liblinear'),\n",
    "        {'C': [0.1, 1.0, 5.0, 10.0], 'penalty': ['l1', 'l2']}, False\n",
    "    ),\n",
    "     \"LinearSVC\": (\n",
    "        LinearSVC(max_iter=2500, random_state=42, class_weight='balanced', dual=False),\n",
    "         {'C': [0.01, 0.1, 1.0, 5.0, 10.0], 'penalty': ['l1', 'l2']}, False\n",
    "     ),\n",
    "    \"RandomForest\": (\n",
    "        RandomForestClassifier(random_state=42, class_weight='balanced', n_jobs=-1),\n",
    "        {'n_estimators': randint(100, 300), 'max_depth': [10, 20, 30, 40, None], 'min_samples_split': randint(2, 15), 'min_samples_leaf': randint(1, 10), 'bootstrap': [True, False]}, True, 25\n",
    "    ),\n",
    "     \"XGBoost\": (\n",
    "         xgb.XGBClassifier(random_state=42, tree_method='hist', objective='multi:softmax', num_class=len(target_names), eval_metric='mlogloss', early_stopping_rounds=10, use_label_encoder=False, error_score='raise', device='cuda'),\n",
    "         {'n_estimators': randint(150, 500), 'max_depth': randint(4, 12), 'learning_rate': uniform(0.01, 0.2), 'subsample': uniform(0.6, 0.4), 'colsample_bytree': uniform(0.6, 0.4), 'gamma': uniform(0, 0.5), 'min_child_weight': randint(1, 8)}, True, 25\n",
    "     ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MRFb4NRLL_Ya"
   },
   "outputs": [],
   "source": [
    "for fs_name, (X_train_fs, X_val_fs) in feature_sets.items():\n",
    "    print(f\"\\n{'='*25} Experimenting with Feature Set: {fs_name} {'='*25}\")\n",
    "    results[fs_name] = {}\n",
    "\n",
    "    for model_name, (model_instance, params, use_random, *n_iter_val) in models_to_try.items():\n",
    "        fit_params = {}\n",
    "        X_train_model = X_train_fs\n",
    "        X_val_model = X_val_fs\n",
    "\n",
    "        n_iter = n_iter_val[0] if n_iter_val else 20\n",
    "        best_model_tuned, f1_val_score = train_evaluate_model(\n",
    "            model_instance, params, X_train_model, y_train, X_val_model, y_val,\n",
    "            use_random_search=use_random, n_iter=n_iter\n",
    "        )\n",
    "        results[fs_name][model_name] = {'model': best_model_tuned, 'f1_macro': f1_val_score}\n",
    "\n",
    "\n",
    "print(f\"\\n{'='*30} Final Results Summary {'='*30}\")\n",
    "summary_df = pd.DataFrame({\n",
    "    fs: {model: results[fs].get(model, {}).get('f1_macro', 0.0) for model in models_to_try.keys()}\n",
    "    for fs in results.keys()\n",
    "}).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gVGsjGiEOqZ7"
   },
   "source": [
    "### Model Summary and Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QfC_AYa2MFZE"
   },
   "outputs": [],
   "source": [
    "summary_df = summary_df.fillna(0.0)\n",
    "summary_df = summary_df[summary_df.max().sort_values(ascending=False).index]\n",
    "print(summary_df.round(4))\n",
    "\n",
    "best_overall_f1 = summary_df.max().max()\n",
    "if best_overall_f1 > 0:\n",
    "    best_combination = summary_df.stack().idxmax()\n",
    "    best_fs, best_model_name = best_combination\n",
    "    print(f\"\\nBest combination: Feature Set='{best_fs}', Model='{best_model_name}' with F1-macro = {best_overall_f1:.4f}\")\n",
    "\n",
    "    final_model = results[best_fs][best_model_name]['model']\n",
    "    filename = f'best_cultural_classifier_NOtransformer_{best_fs}_{best_model_name}.joblib'\n",
    "    save_path = os.path.join(models_path, filename)\n",
    "    try:\n",
    "        joblib.dump(final_model, save_path)\n",
    "        print(f\"Saved best model to {save_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model: {e}\")\n",
    "else:\n",
    "    print(\"\\nNo valid results found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wLudqB-KMiuw"
   },
   "outputs": [],
   "source": [
    "def save_models_and_print_data(results):\n",
    "    for feature_set, models in results.items():\n",
    "        for model_name, data in models.items():\n",
    "            model = data['model']\n",
    "            f1_macro = data['f1_macro']\n",
    "            fs_clean = feature_set.replace(' ','_').replace('(','').replace(')','').replace('+','_')\n",
    "            filename = f'{model_name}_{fs_clean}.joblib'\n",
    "            save_path = os.path.join(models_path, filename)\n",
    "            try:\n",
    "                joblib.dump(model, save_path)\n",
    "                print(f\"Model: {model_name}, Feature Set: {feature_set}, F1-macro = {f1_macro:.4f}, Saved to: {save_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving {model_name} for {feature_set}: {e}\")\n",
    "\n",
    "print(\"\\n--- Saving All Trained Models ---\")\n",
    "save_models_and_print_data(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DglGWkd0g7RX"
   },
   "source": [
    "### Post-Training Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXBQtQlf-M8H"
   },
   "source": [
    "The following code tests the XGBoost on hybrid features, since it's our model of choice given its performance. It's possible to test the others by changing the names of the feature set and the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vs4S5BCshEvJ"
   },
   "outputs": [],
   "source": [
    "loaded_model = results[\"Hybrid (TFIDF+NERLEX+WD)\"][\"XGBoost\"]['model']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LuLPN_lug6rB"
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- Starting Prediction on Unlabeled Dataset ---\")\n",
    "\n",
    "unlabeled_file_path = os.path.join(data_path, 'test_unlabeled.csv')\n",
    "output_file_path = os.path.join(results_path, 'test_unlabeled_predictions_NOtransformer.csv')\n",
    "\n",
    "required_objects = [\n",
    "    'loaded_model', 'tfidf_vectorizer', 'vectorizer_nerlex',\n",
    "    'imputer_numerical', 'scaler_wd', 'imputer_binary',\n",
    "    'nlp', 'wikidata_client', 'wikidata_cache', 'inverse_label_map'\n",
    "]\n",
    "for obj_name in required_objects:\n",
    "    if obj_name not in locals():\n",
    "        print(f\"ERROR: Required object '{obj_name}' not found in the current environment.\")\n",
    "        print(\"Please ensure the previous cells (data loading, preprocessing, model training/loading) have been run successfully.\")\n",
    "        raise NameError(f\"Object '{obj_name}' is not defined.\")\n",
    "\n",
    "print(f\"Input unlabeled data: {unlabeled_file_path}\")\n",
    "print(f\"Output prediction file: {output_file_path}\")\n",
    "\n",
    "try:\n",
    "    test_df = pd.read_csv(unlabeled_file_path)\n",
    "    print(f\"Successfully loaded unlabeled data. Shape: {test_df.shape}\")\n",
    "    print(\"Sample of unlabeled data:\")\n",
    "    print(test_df.head())\n",
    "    print(\"Columns:\", test_df.columns.tolist())\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Unlabeled data file not found at {unlabeled_file_path}\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to load unlabeled data. {e}\")\n",
    "    raise\n",
    "\n",
    "\n",
    "print(\"\\n1. Combining text fields...\")\n",
    "if 'combined_text' not in test_df.columns:\n",
    "    expected_text_cols = ['name', 'description', 'type', 'category', 'subcategory']\n",
    "    for col in expected_text_cols:\n",
    "        if col not in test_df.columns:\n",
    "            print(f\"Warning: Column '{col}' not found in unlabeled data. Filling with empty strings.\")\n",
    "            test_df[col] = ''\n",
    "    test_df['combined_text'] = test_df.apply(combine_text, axis=1)\n",
    "else:\n",
    "    print(\"'combined_text' column already exists.\")\n",
    "\n",
    "print(\"\\n2. Extracting Wikidata QIDs...\")\n",
    "if 'item' not in test_df.columns:\n",
    "     print(\"ERROR: 'item' column (containing Wikidata URLs) is missing from the unlabeled data.\")\n",
    "     raise KeyError(\"'item' column not found.\")\n",
    "if 'qid' not in test_df.columns:\n",
    "    test_df['qid'] = test_df['item'].apply(get_qid_from_url)\n",
    "else:\n",
    "    print(\"'qid' column already exists.\")\n",
    "\n",
    "\n",
    "print(\"\\n3. Fetching/Applying Wikidata features...\")\n",
    "wikidata_cache_hits_test = 0\n",
    "wikidata_newly_fetched_test = 0\n",
    "wikidata_fetch_errors_test = 0\n",
    "\n",
    "def apply_wikidata_features_test(qid_series):\n",
    "    global wikidata_cache_hits_test, wikidata_newly_fetched_test, wikidata_fetch_errors_test, wikidata_cache\n",
    "    processed_features = []\n",
    "    qids_to_fetch = []\n",
    "\n",
    "    for qid in qid_series:\n",
    "        if qid and qid in wikidata_cache:\n",
    "            processed_features.append(wikidata_cache[qid])\n",
    "            wikidata_cache_hits_test += 1\n",
    "        elif qid:\n",
    "             qids_to_fetch.append(qid)\n",
    "             processed_features.append(None)\n",
    "        else:\n",
    "             processed_features.append({'sitelink_count': 0, 'has_country_of_origin': 0, 'item_age': np.nan})\n",
    "\n",
    "    if qids_to_fetch:\n",
    "        unique_qids_to_fetch = list(set(qids_to_fetch))\n",
    "        print(f\"Identified {len(unique_qids_to_fetch)} unique QIDs needing fetch for the test set...\")\n",
    "        if wikidata_client:\n",
    "            start_fetch_time = time.time()\n",
    "            for qid_to_fetch in unique_qids_to_fetch:\n",
    "                 if qid_to_fetch not in wikidata_cache:\n",
    "                    fetched_data = get_wikidata_features(qid_to_fetch)\n",
    "                    if fetched_data['sitelink_count']==0 and fetched_data['has_country_of_origin']==0 and np.isnan(fetched_data['item_age']):\n",
    "                         wikidata_fetch_errors_test += 1\n",
    "                    wikidata_newly_fetched_test += 1\n",
    "\n",
    "            print(f\"Fetching/Cache check took {time.time() - start_fetch_time:.2f}s\")\n",
    "        else:\n",
    "            print(\"Warning: Wikidata client not available. Wikidata features will be default/missing.\")\n",
    "            for qid_missing in unique_qids_to_fetch:\n",
    "                wikidata_cache[qid_missing] = {'sitelink_count': 0, 'has_country_of_origin': 0, 'item_age': np.nan}\n",
    "\n",
    "\n",
    "    final_features_list = []\n",
    "    for i, qid in enumerate(qid_series):\n",
    "        if processed_features[i] is not None and not qid:\n",
    "            final_features_list.append(processed_features[i])\n",
    "        elif qid and qid in wikidata_cache:\n",
    "             final_features_list.append(wikidata_cache[qid])\n",
    "        elif qid:\n",
    "             final_features_list.append({'sitelink_count': 0, 'has_country_of_origin': 0, 'item_age': np.nan})\n",
    "        else:\n",
    "             final_features_list.append({'sitelink_count': 0, 'has_country_of_origin': 0, 'item_age': np.nan})\n",
    "\n",
    "    print(f\"Test Wikidata summary: Cache Hits={wikidata_cache_hits_test}, Newly Fetched={wikidata_newly_fetched_test}, Fetch Errors={wikidata_fetch_errors_test}\")\n",
    "    if CACHE_FILE and wikidata_newly_fetched_test > 0:\n",
    "        try:\n",
    "            with open(CACHE_FILE, 'wb') as f:\n",
    "                pickle.dump(wikidata_cache, f)\n",
    "            print(f\"Wikidata cache updated and saved to {CACHE_FILE}, now {len(wikidata_cache)} entries.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving updated cache: {e}\")\n",
    "\n",
    "    return final_features_list\n",
    "\n",
    "test_wiki_list = apply_wikidata_features_test(test_df['qid'])\n",
    "test_wiki_df = pd.json_normalize(test_wiki_list).add_prefix('wd_')\n",
    "\n",
    "required_wd_cols = ['wd_sitelink_count', 'wd_has_country_of_origin', 'wd_item_age']\n",
    "for col in required_wd_cols:\n",
    "    if col not in test_wiki_df.columns:\n",
    "         print(f\"Warning: Column '{col}' missing after WD fetch for test set. Adding placeholder.\")\n",
    "         if 'count' in col or 'has' in col:\n",
    "             test_wiki_df[col] = 0\n",
    "         else:\n",
    "             test_wiki_df[col] = np.nan\n",
    "\n",
    "test_df = pd.concat([test_df.reset_index(drop=True), test_wiki_df.reset_index(drop=True)], axis=1)\n",
    "print(\"Wikidata features merged into test DataFrame.\")\n",
    "\n",
    "print(\"\\n4. Imputing and Scaling Wikidata features...\")\n",
    "wikidata_numerical_cols = ['wd_sitelink_count', 'wd_item_age']\n",
    "wikidata_binary_cols = ['wd_has_country_of_origin']\n",
    "\n",
    "try:\n",
    "    missing_num = [c for c in wikidata_numerical_cols if c not in test_df.columns]\n",
    "    missing_bin = [c for c in wikidata_binary_cols if c not in test_df.columns]\n",
    "    if missing_num: raise ValueError(f\"Missing numerical WD columns for imputation: {missing_num}\")\n",
    "    if missing_bin: raise ValueError(f\"Missing binary WD columns for imputation: {missing_bin}\")\n",
    "\n",
    "    test_df[wikidata_numerical_cols] = imputer_numerical.transform(test_df[wikidata_numerical_cols])\n",
    "    test_df[wikidata_binary_cols] = imputer_binary.transform(test_df[wikidata_binary_cols])\n",
    "    X_test_wd_numeric_scaled = scaler_wd.transform(test_df[wikidata_numerical_cols])\n",
    "    X_test_wd_binary = test_df[wikidata_binary_cols].values\n",
    "    print(\"WD features imputed and scaled using fitted transformers.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR during WD feature transformation for test set: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"\\n5. Extracting NER+Lexicon features...\")\n",
    "start_time = time.time()\n",
    "test_features_list_nerlex = test_df['combined_text'].apply(extract_ner_lexicon_features).tolist()\n",
    "print(f\"NER/Lexicon feature extraction for test set took {time.time() - start_time:.2f}s\")\n",
    "\n",
    "try:\n",
    "    X_test_nerlex = vectorizer_nerlex.transform(test_features_list_nerlex)\n",
    "    print(f\"Test NER+Lexicon Matrix Shape: {X_test_nerlex.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR transforming NER/Lexicon features for test set: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"\\n6. Transforming text with TF-IDF vectorizer...\")\n",
    "try:\n",
    "    X_test_tfidf = tfidf_vectorizer.transform(test_df['combined_text'])\n",
    "    print(f\"Test TF-IDF Matrix Shape: {X_test_tfidf.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR transforming TF-IDF features for test set: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"\\n7. Combining all features for the test set...\")\n",
    "try:\n",
    "    X_test_hybrid = hstack([\n",
    "        X_test_tfidf,\n",
    "        X_test_nerlex,\n",
    "        X_test_wd_numeric_scaled,\n",
    "        X_test_wd_binary\n",
    "    ], format='csr')\n",
    "    print(f\"Final Test Hybrid Feature Matrix Shape: {X_test_hybrid.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error combining features with hstack for test set: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"\\n8. Making predictions on the test set...\")\n",
    "try:\n",
    "    test_predictions_ids = loaded_model.predict(X_test_hybrid)\n",
    "    print(\"Predictions completed.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR during prediction on test set: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"\\n9. Adding predictions to the DataFrame...\")\n",
    "try:\n",
    "    test_df['predicted_label_id'] = test_predictions_ids\n",
    "    test_df['label'] = test_df['predicted_label_id'].map(inverse_label_map)\n",
    "\n",
    "    if test_df['label'].isnull().any():\n",
    "        print(\"Warning: Some predicted label IDs could not be mapped back to string labels.\")\n",
    "        print(\"Unique predicted IDs:\", test_df['predicted_label_id'].unique())\n",
    "        print(\"Inverse label map:\", inverse_label_map)\n",
    "\n",
    "    print(\"Sample of test data with predictions:\")\n",
    "    print(test_df[['item', 'name', 'predicted_label_id', 'label']].head())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR adding predictions to DataFrame: {e}\")\n",
    "    raise\n",
    "\n",
    "print(f\"\\n10. Saving predictions to {output_file_path}...\")\n",
    "try:\n",
    "    original_cols_df = pd.read_csv(unlabeled_file_path)\n",
    "    output_columns = list(original_cols_df.columns) + ['label']\n",
    "\n",
    "    if 'label' not in output_columns:\n",
    "        output_columns.append('label')\n",
    "\n",
    "    output_columns_present = [col for col in output_columns if col in test_df.columns]\n",
    "    missing_output_cols = [col for col in output_columns if col not in test_df.columns]\n",
    "    if missing_output_cols:\n",
    "        print(f\"Warning: The following requested output columns are missing and will not be saved: {missing_output_cols}\")\n",
    "\n",
    "    test_df.to_csv(output_file_path, columns=output_columns_present, index=False)\n",
    "    print(\"Predictions saved successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR saving prediction file: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"\\n--- Prediction on Unlabeled Dataset Finished ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFJiO8XvOxiZ"
   },
   "source": [
    "## Standalone Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xqi0X7crMwzf"
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- Starting Validation Loop for a specific XGBoost Hybrid Model ---\")\n",
    "\n",
    "model_load_path = os.path.join(models_path, \"XGBoost_Hybrid.joblib\")\n",
    "\n",
    "print(f\"Attempting to load model from: {model_load_path}\")\n",
    "try:\n",
    "    if not os.path.exists(model_load_path):\n",
    "         print(f\"ERROR: Model file not found at {model_load_path}. Please ensure the file exists at this exact path in your Google Drive.\")\n",
    "         raise FileNotFoundError(f\"Model file not found: {model_load_path}\")\n",
    "\n",
    "    loaded_model = joblib.load(model_load_path)\n",
    "    print(f\"Model loaded successfully. Model Type: {type(loaded_model)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to load the model. {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b-JGx8_LOAw5"
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- Starting Prediction on Unlabeled Dataset ---\")\n",
    "\n",
    "unlabeled_file_path = os.path.join(data_path, 'test_unlabeled.csv')\n",
    "output_file_path = os.path.join(results_path, 'test_unlabeled_predictions_NOtransformer.csv')\n",
    "\n",
    "required_objects = [\n",
    "    'loaded_model', 'tfidf_vectorizer', 'vectorizer_nerlex',\n",
    "    'imputer_numerical', 'scaler_wd', 'imputer_binary',\n",
    "    'nlp', 'wikidata_client', 'wikidata_cache', 'inverse_label_map'\n",
    "]\n",
    "for obj_name in required_objects:\n",
    "    if obj_name not in locals():\n",
    "        print(f\"ERROR: Required object '{obj_name}' not found in the current environment.\")\n",
    "        print(\"Please ensure the previous cells (data loading, preprocessing, model training/loading) have been run successfully.\")\n",
    "        raise NameError(f\"Object '{obj_name}' is not defined.\")\n",
    "\n",
    "print(f\"Using model loaded from: {model_load_path}\")\n",
    "print(f\"Input unlabeled data: {unlabeled_file_path}\")\n",
    "print(f\"Output prediction file: {output_file_path}\")\n",
    "\n",
    "try:\n",
    "    test_df = pd.read_csv(unlabeled_file_path)\n",
    "    print(f\"Successfully loaded unlabeled data. Shape: {test_df.shape}\")\n",
    "    print(\"Sample of unlabeled data:\")\n",
    "    print(test_df.head())\n",
    "    print(\"Columns:\", test_df.columns.tolist())\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Unlabeled data file not found at {unlabeled_file_path}\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to load unlabeled data. {e}\")\n",
    "    raise\n",
    "\n",
    "\n",
    "print(\"\\n1. Combining text fields...\")\n",
    "if 'combined_text' not in test_df.columns:\n",
    "    expected_text_cols = ['name', 'description', 'type', 'category', 'subcategory']\n",
    "    for col in expected_text_cols:\n",
    "        if col not in test_df.columns:\n",
    "            print(f\"Warning: Column '{col}' not found in unlabeled data. Filling with empty strings.\")\n",
    "            test_df[col] = ''\n",
    "    test_df['combined_text'] = test_df.apply(combine_text, axis=1)\n",
    "else:\n",
    "    print(\"'combined_text' column already exists.\")\n",
    "\n",
    "print(\"\\n2. Extracting Wikidata QIDs...\")\n",
    "if 'item' not in test_df.columns:\n",
    "     print(\"ERROR: 'item' column (containing Wikidata URLs) is missing from the unlabeled data.\")\n",
    "     raise KeyError(\"'item' column not found.\")\n",
    "if 'qid' not in test_df.columns:\n",
    "    test_df['qid'] = test_df['item'].apply(get_qid_from_url)\n",
    "else:\n",
    "    print(\"'qid' column already exists.\")\n",
    "\n",
    "\n",
    "print(\"\\n3. Fetching/Applying Wikidata features...\")\n",
    "wikidata_cache_hits_test = 0\n",
    "wikidata_newly_fetched_test = 0\n",
    "wikidata_fetch_errors_test = 0\n",
    "\n",
    "def apply_wikidata_features_test(qid_series):\n",
    "    global wikidata_cache_hits_test, wikidata_newly_fetched_test, wikidata_fetch_errors_test, wikidata_cache\n",
    "    processed_features = []\n",
    "    qids_to_fetch = []\n",
    "\n",
    "    for qid in qid_series:\n",
    "        if qid and qid in wikidata_cache:\n",
    "            processed_features.append(wikidata_cache[qid])\n",
    "            wikidata_cache_hits_test += 1\n",
    "        elif qid:\n",
    "             qids_to_fetch.append(qid)\n",
    "             processed_features.append(None)\n",
    "        else:\n",
    "             processed_features.append({'sitelink_count': 0, 'has_country_of_origin': 0, 'item_age': np.nan})\n",
    "\n",
    "    if qids_to_fetch:\n",
    "        unique_qids_to_fetch = list(set(qids_to_fetch))\n",
    "        print(f\"Identified {len(unique_qids_to_fetch)} unique QIDs needing fetch for the test set...\")\n",
    "        if wikidata_client:\n",
    "            start_fetch_time = time.time()\n",
    "            for qid_to_fetch in unique_qids_to_fetch:\n",
    "                 if qid_to_fetch not in wikidata_cache:\n",
    "                    fetched_data = get_wikidata_features(qid_to_fetch)\n",
    "                    if fetched_data['sitelink_count']==0 and fetched_data['has_country_of_origin']==0 and np.isnan(fetched_data['item_age']):\n",
    "                         wikidata_fetch_errors_test += 1\n",
    "                    wikidata_newly_fetched_test += 1\n",
    "\n",
    "            print(f\"Fetching/Cache check took {time.time() - start_fetch_time:.2f}s\")\n",
    "        else:\n",
    "            print(\"Warning: Wikidata client not available. Wikidata features will be default/missing.\")\n",
    "            for qid_missing in unique_qids_to_fetch:\n",
    "                wikidata_cache[qid_missing] = {'sitelink_count': 0, 'has_country_of_origin': 0, 'item_age': np.nan}\n",
    "\n",
    "\n",
    "    final_features_list = []\n",
    "    for i, qid in enumerate(qid_series):\n",
    "        if processed_features[i] is not None and not qid:\n",
    "            final_features_list.append(processed_features[i])\n",
    "        elif qid and qid in wikidata_cache:\n",
    "             final_features_list.append(wikidata_cache[qid])\n",
    "        elif qid:\n",
    "             final_features_list.append({'sitelink_count': 0, 'has_country_of_origin': 0, 'item_age': np.nan})\n",
    "        else:\n",
    "             final_features_list.append({'sitelink_count': 0, 'has_country_of_origin': 0, 'item_age': np.nan})\n",
    "\n",
    "    print(f\"Test Wikidata summary: Cache Hits={wikidata_cache_hits_test}, Newly Fetched={wikidata_newly_fetched_test}, Fetch Errors={wikidata_fetch_errors_test}\")\n",
    "    if CACHE_FILE and wikidata_newly_fetched_test > 0:\n",
    "        try:\n",
    "            with open(CACHE_FILE, 'wb') as f:\n",
    "                pickle.dump(wikidata_cache, f)\n",
    "            print(f\"Wikidata cache updated and saved to {CACHE_FILE}, now {len(wikidata_cache)} entries.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving updated cache: {e}\")\n",
    "\n",
    "    return final_features_list\n",
    "\n",
    "test_wiki_list = apply_wikidata_features_test(test_df['qid'])\n",
    "test_wiki_df = pd.json_normalize(test_wiki_list).add_prefix('wd_')\n",
    "\n",
    "required_wd_cols = ['wd_sitelink_count', 'wd_has_country_of_origin', 'wd_item_age']\n",
    "for col in required_wd_cols:\n",
    "    if col not in test_wiki_df.columns:\n",
    "         print(f\"Warning: Column '{col}' missing after WD fetch for test set. Adding placeholder.\")\n",
    "         if 'count' in col or 'has' in col:\n",
    "             test_wiki_df[col] = 0\n",
    "         else:\n",
    "             test_wiki_df[col] = np.nan\n",
    "\n",
    "test_df = pd.concat([test_df.reset_index(drop=True), test_wiki_df.reset_index(drop=True)], axis=1)\n",
    "print(\"Wikidata features merged into test DataFrame.\")\n",
    "\n",
    "print(\"\\n4. Imputing and Scaling Wikidata features...\")\n",
    "wikidata_numerical_cols = ['wd_sitelink_count', 'wd_item_age']\n",
    "wikidata_binary_cols = ['wd_has_country_of_origin']\n",
    "\n",
    "try:\n",
    "    missing_num = [c for c in wikidata_numerical_cols if c not in test_df.columns]\n",
    "    missing_bin = [c for c in wikidata_binary_cols if c not in test_df.columns]\n",
    "    if missing_num: raise ValueError(f\"Missing numerical WD columns for imputation: {missing_num}\")\n",
    "    if missing_bin: raise ValueError(f\"Missing binary WD columns for imputation: {missing_bin}\")\n",
    "\n",
    "    test_df[wikidata_numerical_cols] = imputer_numerical.transform(test_df[wikidata_numerical_cols])\n",
    "    test_df[wikidata_binary_cols] = imputer_binary.transform(test_df[wikidata_binary_cols])\n",
    "    X_test_wd_numeric_scaled = scaler_wd.transform(test_df[wikidata_numerical_cols])\n",
    "    X_test_wd_binary = test_df[wikidata_binary_cols].values\n",
    "    print(\"WD features imputed and scaled using fitted transformers.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR during WD feature transformation for test set: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"\\n5. Extracting NER+Lexicon features...\")\n",
    "start_time = time.time()\n",
    "test_features_list_nerlex = test_df['combined_text'].apply(extract_ner_lexicon_features).tolist()\n",
    "print(f\"NER/Lexicon feature extraction for test set took {time.time() - start_time:.2f}s\")\n",
    "\n",
    "try:\n",
    "    X_test_nerlex = vectorizer_nerlex.transform(test_features_list_nerlex)\n",
    "    print(f\"Test NER+Lexicon Matrix Shape: {X_test_nerlex.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR transforming NER/Lexicon features for test set: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"\\n6. Transforming text with TF-IDF vectorizer...\")\n",
    "try:\n",
    "    X_test_tfidf = tfidf_vectorizer.transform(test_df['combined_text'])\n",
    "    print(f\"Test TF-IDF Matrix Shape: {X_test_tfidf.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR transforming TF-IDF features for test set: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"\\n7. Combining all features for the test set...\")\n",
    "try:\n",
    "    X_test_hybrid = hstack([\n",
    "        X_test_tfidf,\n",
    "        X_test_nerlex,\n",
    "        X_test_wd_numeric_scaled,\n",
    "        X_test_wd_binary\n",
    "    ], format='csr')\n",
    "    print(f\"Final Test Hybrid Feature Matrix Shape: {X_test_hybrid.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error combining features with hstack for test set: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"\\n8. Making predictions on the test set...\")\n",
    "try:\n",
    "    test_predictions_ids = loaded_model.predict(X_test_hybrid)\n",
    "    print(\"Predictions completed.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR during prediction on test set: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"\\n9. Adding predictions to the DataFrame...\")\n",
    "try:\n",
    "    test_df['predicted_label_id'] = test_predictions_ids\n",
    "    test_df['label'] = test_df['predicted_label_id'].map(inverse_label_map)\n",
    "\n",
    "    if test_df['label'].isnull().any():\n",
    "        print(\"Warning: Some predicted label IDs could not be mapped back to string labels.\")\n",
    "        print(\"Unique predicted IDs:\", test_df['predicted_label_id'].unique())\n",
    "        print(\"Inverse label map:\", inverse_label_map)\n",
    "\n",
    "    print(\"Sample of test data with predictions:\")\n",
    "    print(test_df[['item', 'name', 'predicted_label_id', 'label']].head())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR adding predictions to DataFrame: {e}\")\n",
    "    raise\n",
    "\n",
    "print(f\"\\n10. Saving predictions to {output_file_path}...\")\n",
    "try:\n",
    "    original_cols_df = pd.read_csv(unlabeled_file_path)\n",
    "    output_columns = list(original_cols_df.columns) + ['label']\n",
    "\n",
    "    if 'label' not in output_columns:\n",
    "        output_columns.append('label')\n",
    "\n",
    "    output_columns_present = [col for col in output_columns if col in test_df.columns]\n",
    "    missing_output_cols = [col for col in output_columns if col not in test_df.columns]\n",
    "    if missing_output_cols:\n",
    "        print(f\"Warning: The following requested output columns are missing and will not be saved: {missing_output_cols}\")\n",
    "\n",
    "    test_df.to_csv(output_file_path, columns=output_columns_present, index=False)\n",
    "    print(\"Predictions saved successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR saving prediction file: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"\\n--- Prediction on Unlabeled Dataset Finished ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53mNnf7KO6uv"
   },
   "source": [
    "## Additional Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yglMMZOhO873"
   },
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lOTEULzWOJYe"
   },
   "outputs": [],
   "source": [
    "predictions_file_path = output_file_path\n",
    "label_column = 'predicted_label'\n",
    "original_columns_to_analyze = ['type', 'category', 'subcategory']\n",
    "\n",
    "\n",
    "print(f\"--- Analyzing Predictions from: {predictions_file_path} ---\")\n",
    "try:\n",
    "    df_predictions = pd.read_csv(predictions_file_path)\n",
    "    print(f\"Successfully loaded predictions file. Shape: {df_predictions.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Predictions file not found at {predictions_file_path}\")\n",
    "    print(\"Please ensure the previous prediction script ran successfully and the file exists.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to load predictions file. {e}\")\n",
    "    exit()\n",
    "\n",
    "print(\"\\n--- Basic File Information ---\")\n",
    "print(f\"Total number of predictions: {len(df_predictions)}\")\n",
    "print(f\"Columns found: {df_predictions.columns.tolist()}\")\n",
    "\n",
    "if label_column not in df_predictions.columns:\n",
    "    print(f\"ERROR: The expected prediction column '{label_column}' was not found in the file.\")\n",
    "    print(\"Please check the column names in the CSV and update the 'label_column' variable if needed.\")\n",
    "    exit()\n",
    "\n",
    "missing_predictions = df_predictions[label_column].isnull().sum()\n",
    "if missing_predictions > 0:\n",
    "    print(f\"\\nWARNING: Found {missing_predictions} rows with missing predicted labels.\")\n",
    "else:\n",
    "    print(\"\\nNo missing values found in the predicted label column.\")\n",
    "\n",
    "print(f\"\\n--- Distribution of Predicted Labels ({label_column}) ---\")\n",
    "label_counts = df_predictions[label_column].value_counts()\n",
    "label_percentages = df_predictions[label_column].value_counts(normalize=True) * 100\n",
    "\n",
    "distribution_df = pd.DataFrame({\n",
    "    'Count': label_counts,\n",
    "    'Percentage': label_percentages.round(2)\n",
    "})\n",
    "\n",
    "print(distribution_df)\n",
    "\n",
    "print(\"\\n--- Visualizing Label Distribution ---\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df_predictions, y=label_column, order=label_counts.index, palette='viridis')\n",
    "plt.title('Distribution of Predicted Labels (No Transformer Model)')\n",
    "plt.xlabel('Number of Items')\n",
    "plt.ylabel('Predicted Label')\n",
    "for index, value in enumerate(label_counts):\n",
    "    plt.text(value, index, f' {value} ({label_percentages.iloc[index]:.1f}%)', va='center')\n",
    "plt.tight_layout()\n",
    "dist_plot_path = os.path.join(plot_save_dir, 'predicted_label_distribution.png')\n",
    "plt.savefig(dist_plot_path, bbox_inches='tight')\n",
    "print(f\"Saved label distribution plot to {dist_plot_path}\")\n",
    "plt.close()\n",
    "\n",
    "\n",
    "print(f\"\\n--- Sample Predictions per Label ({label_column}) ---\")\n",
    "display_cols = ['item']\n",
    "if 'name' in df_predictions.columns:\n",
    "    display_cols.append('name')\n",
    "if 'description' in df_predictions.columns:\n",
    "    display_cols.append('description')\n",
    "display_cols.append(label_column)\n",
    "\n",
    "display_cols = [col for col in display_cols if col in df_predictions.columns]\n",
    "\n",
    "if len(display_cols) > 1:\n",
    "    try:\n",
    "        sample_predictions = df_predictions.groupby(label_column)[display_cols].head(3)\n",
    "        print(sample_predictions.to_string())\n",
    "    except KeyError as e:\n",
    "         print(f\"Warning: Could not display samples because column {e} is missing.\")\n",
    "    except Exception as e:\n",
    "         print(f\"An error occurred while trying to display samples: {e}\")\n",
    "else:\n",
    "    print(\"Skipping sample display as 'name' or 'description' columns are not found.\")\n",
    "\n",
    "\n",
    "print(f\"\\n--- Cross-tabulation with Original Features (if available) ---\")\n",
    "analyzed_cross_tabs = False\n",
    "for original_col in original_columns_to_analyze:\n",
    "    if original_col in df_predictions.columns:\n",
    "        unique_values = df_predictions[original_col].nunique()\n",
    "        if unique_values > 50:\n",
    "             print(f\"Skipping crosstab for '{original_col}' (Too many unique values: {unique_values})\")\n",
    "             continue\n",
    "        if unique_values < 2:\n",
    "             print(f\"Skipping crosstab for '{original_col}' (Not enough unique values: {unique_values})\")\n",
    "             continue\n",
    "\n",
    "\n",
    "        print(f\"\\n* Predicted Labels vs. '{original_col}':\")\n",
    "        try:\n",
    "            cross_tab = pd.crosstab(df_predictions[original_col].fillna('Unknown'), df_predictions[label_column])\n",
    "            cross_tab_percent = pd.crosstab(df_predictions[original_col].fillna('Unknown'), df_predictions[label_column], normalize='index') * 100\n",
    "\n",
    "            print(\"Counts:\")\n",
    "            print(cross_tab)\n",
    "            print(\"\\nRow Percentages (%):\")\n",
    "            print(cross_tab_percent.round(1))\n",
    "            analyzed_cross_tabs = True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Could not generate crosstab for '{original_col}': {e}\")\n",
    "    else:\n",
    "        print(f\"Column '{original_col}' not found in the predictions file, skipping crosstab.\")\n",
    "\n",
    "if not analyzed_cross_tabs:\n",
    "    print(\"No suitable original columns found or specified for cross-tabulation analysis.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Analysis Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F3ykihzFO_kQ"
   },
   "outputs": [],
   "source": [
    "if 'summary_df' in locals() and isinstance(summary_df, pd.DataFrame) and not summary_df.empty:\n",
    "    print(\"\\n--- Plotting F1-Score Summary ---\")\n",
    "    try:\n",
    "        ax = summary_df.plot(kind='bar', figsize=(14, 8), rot=45)\n",
    "        plt.title('Comparison of Model F1-Macro Scores across Feature Sets')\n",
    "        plt.ylabel('Validation F1-Macro Score')\n",
    "        plt.xlabel('Feature Set')\n",
    "        plt.legend(title='Models', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.grid(axis='y', linestyle='--')\n",
    "        plt.ylim(bottom=max(0, summary_df.min().min() - 0.05))\n",
    "        plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "        f1_plot_path = os.path.join(plot_save_dir, 'f1_summary_comparison.png')\n",
    "        plt.savefig(f1_plot_path, bbox_inches='tight')\n",
    "        print(f\"Saved F1 summary plot to {f1_plot_path}\")\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting F1 summary chart: {e}\")\n",
    "else:\n",
    "    print(\"Skipping F1 summary plot: summary_df not found, is not a DataFrame, or is empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YES-kjSAPGyQ"
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- Checking for XGBoost Training History ---\")\n",
    "if isinstance(loaded_model, xgb.XGBClassifier) and hasattr(loaded_model, 'evals_result'):\n",
    "    try:\n",
    "        results_eval = loaded_model.evals_result()\n",
    "        if results_eval and 'validation_0' in results_eval:\n",
    "            eval_metric = list(results_eval['validation_0'].keys())[0]\n",
    "            epochs = len(results_eval['validation_0'][eval_metric])\n",
    "            x_axis = range(0, epochs)\n",
    "\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(x_axis, results_eval['validation_0'][eval_metric], label='Validation')\n",
    "\n",
    "            plt.legend()\n",
    "            plt.ylabel(f'{eval_metric.capitalize()}')\n",
    "            plt.xlabel('Boosting Rounds')\n",
    "            plt.title(f'XGBoost {eval_metric.capitalize()} History')\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "            history_plot_path = os.path.join(plot_save_dir, 'xgboost_training_history.png')\n",
    "            plt.savefig(history_plot_path, bbox_inches='tight')\n",
    "            print(f\"Plotted and saved XGBoost training history to {history_plot_path}.\")\n",
    "            plt.close()\n",
    "        else:\n",
    "            print(\"No evaluation results found in 'validation_0'. Cannot plot history.\")\n",
    "\n",
    "    except AttributeError:\n",
    "         print(\"The loaded XGBoost model does not have evaluation results stored ('evals_result()').\")\n",
    "         print(\"This usually happens if early stopping wasn't used or the model wasn't trained with an eval_set directly.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting XGBoost history: {e}\")\n",
    "        print(\"Eval results structure:\", results_eval)\n",
    "else:\n",
    "    print(\"Skipping XGBoost history plot: Loaded model is not XGBoost or lacks evaluation results.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aJKPZ2FBPRQ6"
   },
   "outputs": [],
   "source": [
    "if 'y_val_true' in locals() and 'y_val_pred' in locals() and 'target_names' in locals():\n",
    "    print(\"\\n--- Plotting Confusion Matrix ---\")\n",
    "    try:\n",
    "        cm = confusion_matrix(y_val_true, y_val_pred)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=target_names, yticklabels=target_names)\n",
    "        plt.title(f'Confusion Matrix for Loaded Model ({os.path.basename(model_load_path)})')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.tight_layout()\n",
    "        cm_plot_path = os.path.join(plot_save_dir, 'confusion_matrix_validation.png')\n",
    "        plt.savefig(cm_plot_path, bbox_inches='tight')\n",
    "        print(f\"Saved confusion matrix plot to {cm_plot_path}\")\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting confusion matrix: {e}\")\n",
    "else:\n",
    "    print(\"Skipping confusion matrix plot: y_val_true, y_val_pred, or target_names not found.\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "K8eTzqy_Iini",
    "229g6nr-Ohpm",
    "fFJiO8XvOxiZ",
    "53mNnf7KO6uv",
    "yglMMZOhO873"
   ],
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
