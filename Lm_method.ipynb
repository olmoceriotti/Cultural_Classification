{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7YEkb6DCOIb"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3C53ucfvCPzQ"
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets scikit-learn sentence-transformers tqdm lightgbm torch accelerate -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "koIrAwmlCQee"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from google.colab import drive\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer, util, InputExample, losses, models\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint, loguniform\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import gc\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    precision_recall_curve,\n",
    "    average_precision_score\n",
    ")\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "--hlYEUGDbJO"
   },
   "outputs": [],
   "source": [
    "drive.mount('/content/drive', force_remount=True)\n",
    "base_drive_path = '/content/drive/MyDrive/Bert&Ernie_shared_folder/'\n",
    "data_path = os.path.join(base_drive_path, 'data')\n",
    "models_path = os.path.join(base_drive_path, 'models')\n",
    "results_path = os.path.join(base_drive_path, 'results')\n",
    "sbert_finetuned_path = os.path.join(models_path, \"sbert_triplet_finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7cRqYW5ro77m"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LVNUDsYDCT6D"
   },
   "outputs": [],
   "source": [
    "!huggingface-cli login --token #INSERT TOKEN HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zND-CyGqCXpA"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ws4JcposCaE6"
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"sapienzanlp/nlp2025_hw1_cultural_dataset\")['train']\n",
    "\n",
    "wiki_text_train_path = os.path.join(data_path, \"wikipedia_texts_train.csv\")\n",
    "wiki_text_val_path = os.path.join(data_path, \"wikipedia_texts_val.csv\")\n",
    "test_unlabeled_path = os.path.join(data_path, \"test_unlabeled.csv\")\n",
    "test_texts_path = os.path.join(data_path, \"wikipedia_texts_test.csv\")\n",
    "\n",
    "\n",
    "wiki_text_df = pd.read_csv(wiki_text_train_path)\n",
    "\n",
    "id_to_text = dict(zip(wiki_text_df['id'], wiki_text_df['english_text']))\n",
    "\n",
    "none_count = 0\n",
    "for id, text in id_to_text.items():\n",
    "    if text is None:\n",
    "      none_count += 1\n",
    "if none_count > 0:\n",
    "    print(f\"Warning: Found {none_count} entries with None text in train mapping.\")\n",
    "\n",
    "def extract_entity_id(url):\n",
    "    return url.strip().split(\"/\")[-1]\n",
    "\n",
    "def add_text(example):\n",
    "    entity_id = extract_entity_id(example[\"item\"])\n",
    "    text = id_to_text.get(entity_id, \"\")\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\"\n",
    "    return {\"text\": text}\n",
    "\n",
    "print(\"Processing training data...\")\n",
    "dataset = dataset.map(add_text, num_proc=4)\n",
    "train_df = dataset.to_pandas()\n",
    "print(f\"Training data processed. Shape: {train_df.shape}\")\n",
    "print(f\"Number of empty texts in training data: {(train_df['text'] == '').sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ANY9o6h0Cey1"
   },
   "outputs": [],
   "source": [
    "print(\"\\nProcessing validation data...\")\n",
    "val_dataset = load_dataset(\"sapienzanlp/nlp2025_hw1_cultural_dataset\")['validation']\n",
    "wiki_text_df_val = pd.read_csv(wiki_text_val_path)\n",
    "id_to_text_val = dict(zip(wiki_text_df_val['id'], wiki_text_df_val['english_text']))\n",
    "none_count_val = 0\n",
    "for id, text in id_to_text_val.items():\n",
    "    if text is None:\n",
    "      none_count_val += 1\n",
    "if none_count_val > 0:\n",
    "    print(f\"Warning: Found {none_count_val} entries with None text in validation mapping.\")\n",
    "\n",
    "def add_text_val(example):\n",
    "    entity_id = extract_entity_id(example[\"item\"])\n",
    "    text = id_to_text_val.get(entity_id, \"\")\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\"\n",
    "    return {\"text\": text}\n",
    "\n",
    "\n",
    "val_dataset = val_dataset.map(add_text_val, num_proc=4)\n",
    "val_df = val_dataset.to_pandas()\n",
    "print(f\"Validation data processed. Shape: {val_df.shape}\")\n",
    "print(f\"Number of empty texts in validation data: {(val_df['text'] == '').sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SfuN5k2iCl0L"
   },
   "source": [
    "### Embedding Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZuBEEIaHCvc_"
   },
   "source": [
    "#### Embedding Model Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0C3RCIFLCp2R"
   },
   "outputs": [],
   "source": [
    "print(\"\\nGenerating initial embeddings...\")\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "initial_embeddings_train = model.encode(train_df['text'].tolist(), show_progress_bar=True, convert_to_numpy=True)\n",
    "initial_embeddings_val = model.encode(val_df['text'].tolist(), show_progress_bar=True, convert_to_numpy=True)\n",
    "print(\"Initial embeddings generated.\")\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z7qpa_x7C1Vm"
   },
   "outputs": [],
   "source": [
    "def create_triplets(df, label_col='label', text_col='text', n_samples_per_label=200, max_triplets=30000):\n",
    "    print(f\"Creating triplets...\")\n",
    "    triplets = []\n",
    "    label_to_texts = df.groupby(label_col)[text_col].apply(list).to_dict()\n",
    "    labels = list(label_to_texts.keys())\n",
    "    num_labels = len(labels)\n",
    "\n",
    "    if num_labels < 2:\n",
    "        print(\"Warning: Need at least 2 labels to create negative samples for triplets.\")\n",
    "        return []\n",
    "\n",
    "    created_count = 0\n",
    "    for label in tqdm(labels, desc=\"Generating triplets\"):\n",
    "        positives = label_to_texts[label]\n",
    "        if len(positives) < 2:\n",
    "            continue\n",
    "\n",
    "        num_possible_positive_pairs = len(positives) * (len(positives) - 1) // 2\n",
    "        num_triplets_for_label = min(n_samples_per_label, num_possible_positive_pairs)\n",
    "\n",
    "        positive_pairs = []\n",
    "        for i in range(len(positives)):\n",
    "             for j in range(i + 1, len(positives)):\n",
    "                 positive_pairs.append((positives[i], positives[j]))\n",
    "\n",
    "        random.shuffle(positive_pairs)\n",
    "        selected_pairs = positive_pairs[:num_triplets_for_label]\n",
    "\n",
    "        possible_negative_labels = [lbl for lbl in labels if lbl != label]\n",
    "        if not possible_negative_labels: continue\n",
    "\n",
    "        for anchor, positive in selected_pairs:\n",
    "            if created_count >= max_triplets: break\n",
    "\n",
    "            negative_label = random.choice(possible_negative_labels)\n",
    "            if not label_to_texts[negative_label]: continue\n",
    "\n",
    "            negative = random.choice(label_to_texts[negative_label])\n",
    "\n",
    "            if anchor and positive and negative:\n",
    "                 triplets.append(InputExample(texts=[anchor, positive, negative]))\n",
    "                 created_count += 1\n",
    "        if created_count >= max_triplets: break\n",
    "\n",
    "    print(f\"Created {len(triplets)} triplets (capped at {max_triplets}).\")\n",
    "    return triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d77vphyWDu7v"
   },
   "outputs": [],
   "source": [
    "triplet_examples = create_triplets(train_df, n_samples_per_label=500, max_triplets=50000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AWsn6qJ9DxAQ"
   },
   "outputs": [],
   "source": [
    "model_name = \"all-mpnet-base-v2\"\n",
    "sbert_finetuned_path = os.path.join(models_path, \"sbert_triplet_finetuned\")\n",
    "\n",
    "print(f\"\\nLoading base model {model_name} for fine-tuning...\")\n",
    "triplet_model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "irPFiHqvD4KK"
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(triplet_examples, shuffle=True, batch_size=32)\n",
    "train_loss = losses.TripletLoss(model=triplet_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b05-GRPlD6wS"
   },
   "outputs": [],
   "source": [
    "print(\"Starting fine-tuning with TripletLoss...\")\n",
    "triplet_model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=1,\n",
    "    warmup_steps=100,\n",
    "    output_path=sbert_finetuned_path,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "print(f\"Fine-tuning finished. Model saved to: {sbert_finetuned_path}\")\n",
    "del triplet_model\n",
    "del initial_embeddings_train\n",
    "del initial_embeddings_val\n",
    "del triplet_examples\n",
    "del train_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z66zsv1su8xf"
   },
   "source": [
    "#### Generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ecn7MbLMEAHJ"
   },
   "outputs": [],
   "source": [
    "print(\"\\nGenerating embeddings using the fine-tuned model...\")\n",
    "finetuned_model = SentenceTransformer(sbert_finetuned_path)\n",
    "\n",
    "X_train = finetuned_model.encode(train_df['text'].tolist(), show_progress_bar=True, convert_to_numpy=True)\n",
    "X_val = finetuned_model.encode(val_df['text'].tolist(), show_progress_bar=True, convert_to_numpy=True)\n",
    "print(\"Fine-tuned embeddings generated for train and validation sets.\")\n",
    "\n",
    "y_train = train_df['label'].values\n",
    "y_val = val_df['label'].values\n",
    "\n",
    "\n",
    "del finetuned_model\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aDqTI71QEClS"
   },
   "outputs": [],
   "source": [
    "class_labels = sorted(np.unique(y_train))\n",
    "print(f\"Class labels found for plotting: {class_labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FDUwO98nEFDx"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6IyFa3DbEJwF"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UTkvwUMeEMwc"
   },
   "outputs": [],
   "source": [
    "print(\"Training kNN...\")\n",
    "knn = KNeighborsClassifier(n_neighbors=7)\n",
    "knn.fit(X_train, y_train)\n",
    "knn_preds = knn.predict(X_val)\n",
    "print(\"\\nkNN Classification Report:\")\n",
    "print(classification_report(y_val, knn_preds, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OsPUZcBaEQeH"
   },
   "outputs": [],
   "source": [
    "print(\"\\nTraining Logistic Regression...\")\n",
    "logreg = LogisticRegression(max_iter=1500, random_state=42, C=0.5)\n",
    "logreg.fit(X_train, y_train)\n",
    "logreg_preds = logreg.predict(X_val)\n",
    "print(\"\\nLogistic Regression Classification Report:\")\n",
    "print(classification_report(y_val, logreg_preds, zero_division=0))\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "weUdZN05ESrs"
   },
   "outputs": [],
   "source": [
    "print(\"\\nTraining SVM (RBF Kernel)...\")\n",
    "svm_clf = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42, cache_size=500)\n",
    "svm_clf.fit(X_train, y_train)\n",
    "svm_preds = svm_clf.predict(X_val)\n",
    "print(\"\\nSVM (RBF Kernel) Classification Report:\")\n",
    "print(classification_report(y_val, svm_preds, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEiaNjl-EYTN"
   },
   "source": [
    "### Hyperparameter Tuning (Use this to re-train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JoIA7W5eEdKp"
   },
   "outputs": [],
   "source": [
    "N_ITER_SEARCH = 20\n",
    "CV_FOLDS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42pPYmjMEfE2"
   },
   "source": [
    "#### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aIb0xHwFEjiD"
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- Tuning kNN ---\")\n",
    "param_dist_knn = {\n",
    "    'n_neighbors': randint(3, 15),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'cosine']\n",
    "}\n",
    "knn_clf = KNeighborsClassifier()\n",
    "random_search_knn = RandomizedSearchCV(knn_clf, param_distributions=param_dist_knn,\n",
    "                                       n_iter=N_ITER_SEARCH, cv=CV_FOLDS, scoring='f1_macro',\n",
    "                                       n_jobs=-1, random_state=42, verbose=1)\n",
    "random_search_knn.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nBest kNN Parameters:\", random_search_knn.best_params_)\n",
    "print(\"Best kNN Cross-validation F1-Macro Score:\", random_search_knn.best_score_)\n",
    "knn_tuned_preds = random_search_knn.best_estimator_.predict(X_val)\n",
    "print(\"\\nkNN Classification Report (Tuned):\")\n",
    "print(classification_report(y_val, knn_tuned_preds, zero_division=0))\n",
    "del knn_clf\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjplXZwOElUq"
   },
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vWtZ1__AE6Bd"
   },
   "outputs": [],
   "source": [
    "param_dist_logreg = {\n",
    "    'C': loguniform(1e-3, 1e2),\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['saga'],\n",
    "    'max_iter': [2000, 3000, 4000]\n",
    "}\n",
    "logreg_clf = LogisticRegression(random_state=42, n_jobs=-1)\n",
    "random_search_logreg = RandomizedSearchCV(logreg_clf, param_distributions=param_dist_logreg,\n",
    "                                          n_iter=N_ITER_SEARCH, cv=CV_FOLDS, scoring='f1_macro',\n",
    "                                          n_jobs=-1, random_state=42, verbose=1)\n",
    "random_search_logreg.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nBest Logistic Regression Parameters:\", random_search_logreg.best_params_)\n",
    "print(\"Best Logistic Regression Cross-validation F1-Macro Score:\", random_search_logreg.best_score_)\n",
    "logreg_tuned_preds = random_search_logreg.best_estimator_.predict(X_val)\n",
    "print(\"\\nLogistic Regression Classification Report (Tuned):\")\n",
    "print(classification_report(y_val, logreg_tuned_preds, zero_division=0))\n",
    "del logreg_clf\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k707XgRJFAqo"
   },
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QMDx0Uj5FAGs"
   },
   "outputs": [],
   "source": [
    "param_dist_svm = {\n",
    "    'C': loguniform(1e-1, 1e2),\n",
    "    'kernel': ['rbf', 'linear'],\n",
    "    'gamma': ['scale', 'auto'] + list(loguniform(1e-4, 1e-1).rvs(5)),\n",
    "}\n",
    "svm_clf_tune = SVC(random_state=42, cache_size=700, probability=True)\n",
    "random_search_svm = RandomizedSearchCV(svm_clf_tune, param_distributions=param_dist_svm,\n",
    "                                       n_iter=N_ITER_SEARCH // 2, cv=CV_FOLDS, scoring='f1_macro',\n",
    "                                       n_jobs=-1, random_state=42, verbose=1)\n",
    "random_search_svm.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nBest SVM Parameters:\", random_search_svm.best_params_)\n",
    "print(\"Best SVM Cross-validation F1-Macro Score:\", random_search_svm.best_score_)\n",
    "svm_tuned_preds = random_search_svm.best_estimator_.predict(X_val)\n",
    "print(\"\\nSVM Classification Report (Tuned):\")\n",
    "print(classification_report(y_val, svm_tuned_preds, zero_division=0))\n",
    "del svm_clf_tune\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWimwBlhFHF7"
   },
   "source": [
    "#### Model Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-BmilyDiFKi6"
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- Saving Tuned Models ---\")\n",
    "model_save_dir = models_path\n",
    "\n",
    "def save_model(estimator, model_name, search_object_name):\n",
    "    try:\n",
    "        if search_object_name not in globals():\n",
    "             print(f\"'{search_object_name}' not found. Skipping saving {model_name} model.\")\n",
    "             return\n",
    "\n",
    "        best_estimator = globals()[search_object_name].best_estimator_\n",
    "        filename = os.path.join(model_save_dir, f\"best_{model_name}_model.joblib\")\n",
    "        joblib.dump(best_estimator, filename)\n",
    "        print(f\"Saved best {model_name} model to {filename}\")\n",
    "    except AttributeError:\n",
    "         print(f\"'{search_object_name}' does not have 'best_estimator_'. Was tuning run?\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving {model_name} model: {e}\")\n",
    "\n",
    "save_model(KNeighborsClassifier, \"knn\", \"random_search_knn\")\n",
    "save_model(LogisticRegression, \"logreg\", \"random_search_logreg\")\n",
    "save_model(SVC, \"svm\", \"random_search_svm\")\n",
    "\n",
    "\n",
    "print(\"\\nModel saving process finished.\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BCoJywdXfVTe"
   },
   "source": [
    "### Post-training Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z3yh1Uvx-uh2"
   },
   "source": [
    "The following code tests the SVM model, since it's our model of choice given its performance. It's possible to test the others by modifiying the variable loaded_svm_model and fetching the best estimator of the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scENzOn_flMw"
   },
   "outputs": [],
   "source": [
    "print(\"--- Starting Evaluation on Unlabeled Test Set ---\")\n",
    "\n",
    "\n",
    "output_filename = \"test_unlabelled_with_svm_predictions.csv\"\n",
    "output_file_path = os.path.join(results_path, output_filename)\n",
    "\n",
    "model_save_dir = models_path\n",
    "\n",
    "finetuned_sbert_path = os.path.join(models_path, \"sbert_triplet_finetuned\")\n",
    "\n",
    "print(f\"Loading unlabeled test data from: {test_unlabeled_path}\")\n",
    "if not os.path.exists(test_unlabeled_path):\n",
    "    print(f\"Error: File not found - {test_unlabeled_path}\")\n",
    "    raise FileNotFoundError(f\"Required file not found: {test_unlabeled_path}\")\n",
    "test_df = pd.read_csv(test_unlabeled_path)\n",
    "print(f\"Loaded {len(test_df)} rows from test data.\")\n",
    "\n",
    "print(f\"Loading test texts from: {test_texts_path}\")\n",
    "if not os.path.exists(test_texts_path):\n",
    "    print(f\"Error: File not found - {test_texts_path}\")\n",
    "    raise FileNotFoundError(f\"Required file not found: {test_texts_path}\")\n",
    "wiki_text_df_test = pd.read_csv(test_texts_path)\n",
    "print(f\"Loaded {len(wiki_text_df_test)} rows from test texts.\")\n",
    "\n",
    "print(\"Creating text mapping for test data...\")\n",
    "id_to_text_test = dict(zip(wiki_text_df_test['id'], wiki_text_df_test['english_text']))\n",
    "\n",
    "none_count_test = 0\n",
    "for id_key, text_val in id_to_text_test.items():\n",
    "    if text_val is None:\n",
    "        none_count_test += 1\n",
    "if none_count_test > 0:\n",
    "    print(f\"Warning: Found {none_count_test} entries with None text in the test mapping.\")\n",
    "\n",
    "def extract_entity_id(url):\n",
    "    if isinstance(url, str) and \"/\" in url:\n",
    "        return url.strip().split(\"/\")[-1]\n",
    "    return None\n",
    "\n",
    "def add_text_test(row):\n",
    "    entity_id = extract_entity_id(row[\"item\"])\n",
    "    if entity_id:\n",
    "        text = id_to_text_test.get(entity_id, \"\")\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        return text\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "print(\"Mapping texts to the test dataframe...\")\n",
    "tqdm.pandas(desc=\"Adding text column\")\n",
    "test_df['text'] = test_df.progress_apply(add_text_test, axis=1)\n",
    "\n",
    "num_empty_texts_test = (test_df['text'] == '').sum()\n",
    "if num_empty_texts_test > 0:\n",
    "     print(f\"Warning: {num_empty_texts_test} rows in the test set have missing or empty text after mapping.\")\n",
    "print(\"Text mapping complete.\")\n",
    "\n",
    "print(f\"Loading fine-tuned Sentence Transformer from: {finetuned_sbert_path}\")\n",
    "if not os.path.exists(finetuned_sbert_path):\n",
    "     print(f\"Error: Fine-tuned Sentence Transformer directory not found at {finetuned_sbert_path}\")\n",
    "     raise FileNotFoundError(f\"Directory not found: {finetuned_sbert_path}. Ensure the fine-tuning step completed successfully and saved to the correct Drive path.\")\n",
    "try:\n",
    "    eval_sbert_model = SentenceTransformer(finetuned_sbert_path)\n",
    "    print(\"Sentence Transformer loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading Sentence Transformer model: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"Generating embeddings for the test data...\")\n",
    "test_texts = test_df['text'].tolist()\n",
    "test_texts_cleaned = [str(text) if pd.notna(text) else \"\" for text in test_texts]\n",
    "\n",
    "X_test_unlabelled = eval_sbert_model.encode(\n",
    "    test_texts_cleaned,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    batch_size=128\n",
    ")\n",
    "print(f\"Generated {X_test_unlabelled.shape[0]} embeddings with dimension {X_test_unlabelled.shape[1]}.\")\n",
    "\n",
    "del eval_sbert_model\n",
    "del test_texts\n",
    "del test_texts_cleaned\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "loaded_svm_model = random_search_svm.best_estimator_\n",
    "\n",
    "print(\"Making predictions on the test data...\")\n",
    "test_predictions = loaded_svm_model.predict(X_test_unlabelled)\n",
    "print(f\"Generated {len(test_predictions)} predictions.\")\n",
    "\n",
    "print(\"Adding predictions to the test dataframe...\")\n",
    "output_df = test_df.copy()\n",
    "output_df['label'] = test_predictions\n",
    "\n",
    "\n",
    "print(f\"Saving results with predictions to: {output_file_path}\")\n",
    "try:\n",
    "    output_df.to_csv(output_file_path, index=False)\n",
    "    print(\"Output file saved successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving output file: {e}\")\n",
    "\n",
    "del test_df\n",
    "del wiki_text_df_test\n",
    "del id_to_text_test\n",
    "del X_test_unlabelled\n",
    "del loaded_svm_model\n",
    "del test_predictions\n",
    "del output_df\n",
    "gc.collect()\n",
    "\n",
    "print(\"--- Evaluation script finished ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kCBepCVeaf_"
   },
   "source": [
    "## Standalone Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZuOTrB4_Gtl0"
   },
   "outputs": [],
   "source": [
    "print(\"--- Starting Evaluation on Unlabeled Test Set ---\")\n",
    "\n",
    "\n",
    "output_filename = \"test_unlabelled_with_svm_predictions.csv\"\n",
    "output_file_path = os.path.join(results_path, output_filename)\n",
    "\n",
    "\n",
    "model_save_dir = models_path\n",
    "\n",
    "svm_model_filename = \"best_svm_model.joblib\"\n",
    "svm_model_path = os.path.join(model_save_dir, svm_model_filename)\n",
    "\n",
    "finetuned_sbert_path = os.path.join(models_path, \"sbert_triplet_finetuned\")\n",
    "\n",
    "print(f\"Loading unlabeled test data from: {test_unlabeled_path}\")\n",
    "if not os.path.exists(test_unlabeled_path):\n",
    "    print(f\"Error: File not found - {test_unlabeled_path}\")\n",
    "    raise FileNotFoundError(f\"Required file not found: {test_unlabeled_path}\")\n",
    "test_df = pd.read_csv(test_unlabeled_path)\n",
    "print(f\"Loaded {len(test_df)} rows from test data.\")\n",
    "\n",
    "print(f\"Loading test texts from: {test_texts_path}\")\n",
    "if not os.path.exists(test_texts_path):\n",
    "    print(f\"Error: File not found - {test_texts_path}\")\n",
    "    raise FileNotFoundError(f\"Required file not found: {test_texts_path}\")\n",
    "wiki_text_df_test = pd.read_csv(test_texts_path)\n",
    "print(f\"Loaded {len(wiki_text_df_test)} rows from test texts.\")\n",
    "\n",
    "print(\"Creating text mapping for test data...\")\n",
    "id_to_text_test = dict(zip(wiki_text_df_test['id'], wiki_text_df_test['english_text']))\n",
    "\n",
    "none_count_test = 0\n",
    "for id_key, text_val in id_to_text_test.items():\n",
    "    if text_val is None:\n",
    "        none_count_test += 1\n",
    "if none_count_test > 0:\n",
    "    print(f\"Warning: Found {none_count_test} entries with None text in the test mapping.\")\n",
    "\n",
    "def extract_entity_id(url):\n",
    "    if isinstance(url, str) and \"/\" in url:\n",
    "        return url.strip().split(\"/\")[-1]\n",
    "    return None\n",
    "\n",
    "def add_text_test(row):\n",
    "    entity_id = extract_entity_id(row[\"item\"])\n",
    "    if entity_id:\n",
    "        text = id_to_text_test.get(entity_id, \"\")\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        return text\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "print(\"Mapping texts to the test dataframe...\")\n",
    "tqdm.pandas(desc=\"Adding text column\")\n",
    "test_df['text'] = test_df.progress_apply(add_text_test, axis=1)\n",
    "\n",
    "num_empty_texts_test = (test_df['text'] == '').sum()\n",
    "if num_empty_texts_test > 0:\n",
    "     print(f\"Warning: {num_empty_texts_test} rows in the test set have missing or empty text after mapping.\")\n",
    "print(\"Text mapping complete.\")\n",
    "\n",
    "print(f\"Loading fine-tuned Sentence Transformer from: {finetuned_sbert_path}\")\n",
    "if not os.path.exists(finetuned_sbert_path):\n",
    "     print(f\"Error: Fine-tuned Sentence Transformer directory not found at {finetuned_sbert_path}\")\n",
    "     raise FileNotFoundError(f\"Directory not found: {finetuned_sbert_path}. Ensure the fine-tuning step completed successfully and saved to the correct Drive path.\")\n",
    "try:\n",
    "    eval_sbert_model = SentenceTransformer(finetuned_sbert_path)\n",
    "    print(\"Sentence Transformer loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading Sentence Transformer model: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"Generating embeddings for the test data...\")\n",
    "test_texts = test_df['text'].tolist()\n",
    "test_texts_cleaned = [str(text) if pd.notna(text) else \"\" for text in test_texts]\n",
    "\n",
    "X_test_unlabelled = eval_sbert_model.encode(\n",
    "    test_texts_cleaned,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    batch_size=128\n",
    ")\n",
    "print(f\"Generated {X_test_unlabelled.shape[0]} embeddings with dimension {X_test_unlabelled.shape[1]}.\")\n",
    "\n",
    "del eval_sbert_model\n",
    "del test_texts\n",
    "del test_texts_cleaned\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"Loading the fine-tuned SVM model from: {svm_model_path}\")\n",
    "if not os.path.exists(svm_model_path):\n",
    "    print(f\"Error: SVM model file not found at {svm_model_path}\")\n",
    "    print(\"Please ensure the model saving block executed correctly and saved the file to the correct Drive path.\")\n",
    "    raise FileNotFoundError(f\"Model file not found: {svm_model_path}\")\n",
    "\n",
    "try:\n",
    "    loaded_svm_model = joblib.load(svm_model_path)\n",
    "    print(\"Fine-tuned SVM model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading SVM model: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"Making predictions on the test data...\")\n",
    "test_predictions = loaded_svm_model.predict(X_test_unlabelled)\n",
    "print(f\"Generated {len(test_predictions)} predictions.\")\n",
    "\n",
    "print(\"Adding predictions to the test dataframe...\")\n",
    "output_df = test_df.copy()\n",
    "output_df['label'] = test_predictions\n",
    "\n",
    "\n",
    "print(f\"Saving results with predictions to: {output_file_path}\")\n",
    "try:\n",
    "    output_df.to_csv(output_file_path, index=False)\n",
    "    print(\"Output file saved successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving output file: {e}\")\n",
    "\n",
    "del test_df\n",
    "del wiki_text_df_test\n",
    "del id_to_text_test\n",
    "del X_test_unlabelled\n",
    "del loaded_svm_model\n",
    "del test_predictions\n",
    "del output_df\n",
    "gc.collect()\n",
    "\n",
    "print(\"--- Evaluation script finished ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_pSb3JYLFPGs"
   },
   "source": [
    "## Additional Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYJpB9lwFSh5"
   },
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FiZr53T4FVbc"
   },
   "outputs": [],
   "source": [
    "PLOT_DIR = os.path.join(results_path, \"evaluation_plots\")\n",
    "os.makedirs(PLOT_DIR, exist_ok=True)\n",
    "print(f\"Plots will be saved to: {PLOT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "97_cCGVHF8or"
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes, model_name, save_dir=PLOT_DIR):\n",
    "    try:\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "        plt.title(f'Confusion Matrix - {model_name}')\n",
    "        plt.ylabel('Actual Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        filename = os.path.join(save_dir, f\"{model_name}_confusion_matrix.png\")\n",
    "        plt.savefig(filename, bbox_inches='tight')\n",
    "        print(f\"Saved confusion matrix to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting confusion matrix for {model_name}: {e}\")\n",
    "    finally:\n",
    "        plt.close()\n",
    "\n",
    "def plot_multiclass_roc_curve(y_true, y_scores, classes, model_name, score_type=\"probability\", save_dir=PLOT_DIR):\n",
    "    plot_title = f'Multi-class ROC ({score_type.replace(\"_\",\" \").title()}) - {model_name}'\n",
    "    print(f\"Generating ROC curve for {model_name} using {score_type} scores...\")\n",
    "    try:\n",
    "        n_classes = len(classes)\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "\n",
    "        if len(y_scores.shape) > 1 and y_scores.shape[1] != n_classes and n_classes > 1 :\n",
    "             print(f\"Warning: Score shape {y_scores.shape} mismatch with n_classes {n_classes} for ROC. Attempting alignment.\")\n",
    "             unique_true = sorted(np.unique(y_true))\n",
    "             if y_scores.shape[1] == len(unique_true):\n",
    "                 classes = unique_true\n",
    "                 n_classes = len(classes)\n",
    "                 print(f\"Aligned based on unique true labels. New n_classes: {n_classes}\")\n",
    "             else:\n",
    "                  print(\"Cannot align score shapes for ROC. Skipping per-class curves.\")\n",
    "                  try:\n",
    "                       y_true_bin_flat = pd.get_dummies(y_true).values.ravel()\n",
    "                       y_scores_flat = y_scores.ravel()\n",
    "                       if y_true_bin_flat.size == y_scores_flat.size:\n",
    "                            fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true_bin_flat, y_scores_flat)\n",
    "                            roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "                            plt.figure(figsize=(8, 6))\n",
    "                            plt.plot(fpr[\"micro\"], tpr[\"micro\"], label=f'Micro-average ROC (area = {roc_auc[\"micro\"]:0.2f})', color='deeppink', linestyle=':', linewidth=4)\n",
    "                            plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "                            plt.xlim([0.0, 1.0]); plt.ylim([0.0, 1.05])\n",
    "                            plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')\n",
    "                            plt.title(f'Micro-Average ROC - {model_name}')\n",
    "                            plt.legend(loc=\"lower right\")\n",
    "                            filename = os.path.join(save_dir, f\"{model_name}_roc_curve_micro_only.png\")\n",
    "                            plt.savefig(filename, bbox_inches='tight')\n",
    "                            print(f\"Saved Micro-Average ROC curve plot to {filename}\")\n",
    "                            plt.close()\n",
    "                       else:\n",
    "                            print(\"Cannot compute Micro-Average ROC due to size mismatch.\")\n",
    "                  except Exception as micro_e:\n",
    "                       print(f\"Error computing Micro-Average ROC: {micro_e}\")\n",
    "                  return\n",
    "\n",
    "        y_true_bin = pd.get_dummies(y_true, columns=classes).values\n",
    "\n",
    "        valid_classes_for_macro = 0\n",
    "        for i in range(n_classes):\n",
    "             if i < y_true_bin.shape[1]:\n",
    "                 current_scores = y_scores[:, i] if len(y_scores.shape) > 1 else y_scores\n",
    "                 if len(y_scores.shape) == 1 and n_classes == 2 and i == 0:\n",
    "                      fpr[i], tpr[i], roc_auc[i] = np.array([0]), np.array([0]), 0.0\n",
    "                      continue\n",
    "                 elif len(y_scores.shape) == 1 and n_classes == 2 and i == 1:\n",
    "                       current_scores = y_scores\n",
    "\n",
    "                 if i < current_scores.shape[0] and np.sum(y_true_bin[:, i]) > 0 and np.sum(y_true_bin[:, i]) < len(y_true_bin[:, i]):\n",
    "                     try:\n",
    "                         fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], current_scores)\n",
    "                         roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "                         valid_classes_for_macro += 1\n",
    "                     except ValueError as roc_err:\n",
    "                          print(f\"Skipping ROC for class {classes[i]} due to error: {roc_err}\")\n",
    "                          fpr[i], tpr[i], roc_auc[i] = np.array([0]), np.array([0]), 0.0\n",
    "                 else:\n",
    "                      print(f\"Skipping ROC for class {classes[i]} - class not present/only one class in y_true subset or index issue.\")\n",
    "                      fpr[i], tpr[i], roc_auc[i] = np.array([0]), np.array([0]), 0.0\n",
    "             else:\n",
    "                 fpr[i], tpr[i], roc_auc[i] = np.array([0]), np.array([0]), 0.0\n",
    "\n",
    "        try:\n",
    "             y_true_bin_flat = y_true_bin.ravel()\n",
    "             y_scores_flat = y_scores.ravel()\n",
    "             if y_true_bin_flat.size == y_scores_flat.size:\n",
    "                 fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true_bin_flat, y_scores_flat)\n",
    "                 roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "             else:\n",
    "                 print(\"Skipping Micro-Average ROC due to size mismatch after binarization.\")\n",
    "                 roc_auc[\"micro\"] = 0.0\n",
    "                 fpr[\"micro\"], tpr[\"micro\"] = np.array([0]), np.array([0])\n",
    "        except Exception as e:\n",
    "             print(f\"Error calculating Micro-Average ROC: {e}\")\n",
    "             roc_auc[\"micro\"] = 0.0\n",
    "             fpr[\"micro\"], tpr[\"micro\"] = np.array([0]), np.array([0])\n",
    "\n",
    "\n",
    "        if valid_classes_for_macro > 0:\n",
    "            all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes) if i in fpr and len(fpr[i])>0]))\n",
    "            mean_tpr = np.zeros_like(all_fpr)\n",
    "            for i in range(n_classes):\n",
    "                if i in fpr and i in tpr and len(fpr[i]) > 0 and len(tpr[i]) > 0:\n",
    "                    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "            mean_tpr /= valid_classes_for_macro\n",
    "            fpr[\"macro\"] = all_fpr\n",
    "            tpr[\"macro\"] = mean_tpr\n",
    "            roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "        else:\n",
    "            print(\"Skipping Macro-Average ROC: No valid per-class curves computed.\")\n",
    "            roc_auc[\"macro\"] = 0.0\n",
    "            fpr[\"macro\"], tpr[\"macro\"] = np.array([0]), np.array([0])\n",
    "\n",
    "\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        if \"micro\" in roc_auc and roc_auc[\"micro\"] > 0:\n",
    "             plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "                     label=f'Micro-average ROC (area = {roc_auc[\"micro\"]:0.2f})',\n",
    "                     color='deeppink', linestyle=':', linewidth=4)\n",
    "        if \"macro\" in roc_auc and roc_auc[\"macro\"] > 0:\n",
    "             plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "                     label=f'Macro-average ROC (area = {roc_auc[\"macro\"]:0.2f})',\n",
    "                     color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "        colors = plt.cm.get_cmap('tab10', n_classes)\n",
    "        for i, color in zip(range(n_classes), colors(range(n_classes))):\n",
    "            if i in roc_auc and roc_auc[i] > 0:\n",
    "                 plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "                         label=f'ROC Class {classes[i]} (area = {roc_auc[i]:0.2f})')\n",
    "\n",
    "        plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "        plt.xlim([0.0, 1.0]); plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')\n",
    "        plt.title(plot_title)\n",
    "        plt.legend(loc=\"lower right\", fontsize='small')\n",
    "        filename = os.path.join(save_dir, f\"{model_name}_roc_curve.png\")\n",
    "        plt.savefig(filename, bbox_inches='tight')\n",
    "        print(f\"Saved ROC curve plot to {filename}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting ROC curve for {model_name}: {e}\")\n",
    "    finally:\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def plot_multiclass_precision_recall(y_true, y_scores, classes, model_name, score_type=\"probability\", save_dir=PLOT_DIR):\n",
    "    plot_title = f'Multi-class Precision-Recall ({score_type.replace(\"_\",\" \").title()}) - {model_name}'\n",
    "    print(f\"Generating Precision-Recall curve for {model_name} using {score_type} scores...\")\n",
    "    try:\n",
    "        n_classes = len(classes)\n",
    "        precision = dict()\n",
    "        recall = dict()\n",
    "        average_precision = dict()\n",
    "\n",
    "        if len(y_scores.shape) > 1 and y_scores.shape[1] != n_classes and n_classes > 1:\n",
    "             print(f\"Warning: Score shape {y_scores.shape} mismatch with n_classes {n_classes} for PR. Attempting alignment.\")\n",
    "             unique_true = sorted(np.unique(y_true))\n",
    "             if y_scores.shape[1] == len(unique_true):\n",
    "                 classes = unique_true\n",
    "                 n_classes = len(classes)\n",
    "                 print(f\"Aligned based on unique true labels. New n_classes: {n_classes}\")\n",
    "             else:\n",
    "                  print(\"Cannot align score shapes for PR. Skipping per-class curves.\")\n",
    "                  try:\n",
    "                       y_true_bin_flat = pd.get_dummies(y_true).values.ravel()\n",
    "                       y_scores_flat = y_scores.ravel()\n",
    "                       if y_true_bin_flat.size == y_scores_flat.size:\n",
    "                           precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(y_true_bin_flat, y_scores_flat)\n",
    "                           y_true_bin_temp = pd.get_dummies(y_true).values\n",
    "                           if len(y_scores.shape) > 1 and y_true_bin_temp.shape[1] == y_scores.shape[1]:\n",
    "                               average_precision[\"micro\"] = average_precision_score(y_true_bin_temp, y_scores, average=\"micro\")\n",
    "                           else:\n",
    "                               average_precision[\"micro\"] = average_precision_score(y_true_bin_flat, y_scores_flat)\n",
    "\n",
    "\n",
    "                           plt.figure(figsize=(8, 6))\n",
    "                           plt.plot(recall[\"micro\"], precision[\"micro\"], label=f'Micro-average PR (AP = {average_precision[\"micro\"]:0.2f})', color='navy', linestyle=':', linewidth=4)\n",
    "                           plt.xlim([0.0, 1.0]); plt.ylim([0.0, 1.05])\n",
    "                           plt.xlabel('Recall'); plt.ylabel('Precision')\n",
    "                           plt.title(f'Micro-Average Precision-Recall - {model_name}')\n",
    "                           plt.legend(loc=\"best\")\n",
    "                           filename = os.path.join(save_dir, f\"{model_name}_precision_recall_curve_micro_only.png\")\n",
    "                           plt.savefig(filename, bbox_inches='tight')\n",
    "                           print(f\"Saved Micro-Average PR curve plot to {filename}\")\n",
    "                           plt.close()\n",
    "                       else:\n",
    "                            print(\"Cannot compute Micro-Average PR due to size mismatch.\")\n",
    "                  except Exception as micro_e:\n",
    "                       print(f\"Error computing Micro-Average PR: {micro_e}\")\n",
    "                  return\n",
    "\n",
    "        y_true_bin = pd.get_dummies(y_true, columns=classes).values\n",
    "\n",
    "        valid_classes_count = 0\n",
    "        for i in range(n_classes):\n",
    "             if i < y_true_bin.shape[1]:\n",
    "                 current_scores = y_scores[:, i] if len(y_scores.shape) > 1 else y_scores\n",
    "                 if len(y_scores.shape) == 1 and n_classes == 2 and i == 0:\n",
    "                       precision[i], recall[i], average_precision[i] = np.array([0]), np.array([1]), 0.0\n",
    "                       continue\n",
    "                 elif len(y_scores.shape) == 1 and n_classes == 2 and i == 1:\n",
    "                       current_scores = y_scores\n",
    "\n",
    "                 if i < current_scores.shape[0] and np.sum(y_true_bin[:, i]) > 0:\n",
    "                     try:\n",
    "                         precision[i], recall[i], _ = precision_recall_curve(y_true_bin[:, i], current_scores)\n",
    "                         average_precision[i] = average_precision_score(y_true_bin[:, i], current_scores)\n",
    "                         valid_classes_count += 1\n",
    "                     except ValueError as pr_err:\n",
    "                         print(f\"Skipping PR for class {classes[i]} due to error: {pr_err}\")\n",
    "                         precision[i], recall[i], average_precision[i] = np.array([0]), np.array([1]), 0.0\n",
    "\n",
    "                 else:\n",
    "                     print(f\"Skipping PR for class {classes[i]} - no positive samples or index issue.\")\n",
    "                     precision[i], recall[i], average_precision[i] = np.array([0]), np.array([1]), 0.0\n",
    "             else:\n",
    "                  precision[i], recall[i], average_precision[i] = np.array([0]), np.array([1]), 0.0\n",
    "\n",
    "        try:\n",
    "             y_true_bin_flat = y_true_bin.ravel()\n",
    "             y_scores_flat = y_scores.ravel()\n",
    "             if y_true_bin_flat.size == y_scores_flat.size:\n",
    "                 precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(y_true_bin_flat, y_scores_flat)\n",
    "                 if len(y_scores.shape) > 1 and y_true_bin.shape[1] == y_scores.shape[1]:\n",
    "                     average_precision[\"micro\"] = average_precision_score(y_true_bin, y_scores, average=\"micro\")\n",
    "                 else:\n",
    "                     average_precision[\"micro\"] = average_precision_score(y_true_bin_flat, y_scores_flat)\n",
    "             else:\n",
    "                 print(\"Skipping Micro-Average PR due to size mismatch after binarization.\")\n",
    "                 average_precision[\"micro\"] = 0.0\n",
    "                 precision[\"micro\"], recall[\"micro\"] = np.array([0]), np.array([1])\n",
    "        except Exception as e:\n",
    "             print(f\"Error calculating Micro-Average PR: {e}\")\n",
    "             average_precision[\"micro\"] = 0.0\n",
    "             precision[\"micro\"], recall[\"micro\"] = np.array([0]), np.array([1])\n",
    "\n",
    "\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        if \"micro\" in average_precision and average_precision[\"micro\"] > 0:\n",
    "             plt.step(recall['micro'], precision['micro'], where='post', label=f'Micro-average PR (AP = {average_precision[\"micro\"]:0.2f})', color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "        colors = plt.cm.get_cmap('tab10', n_classes)\n",
    "        for i, color in zip(range(n_classes), colors(range(n_classes))):\n",
    "             if i in average_precision and average_precision[i] > 0:\n",
    "                 plt.step(recall[i], precision[i], where='post', color=color, lw=2, label=f'PR Class {classes[i]} (AP = {average_precision[i]:0.2f})')\n",
    "\n",
    "        plt.xlim([0.0, 1.0]); plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('Recall'); plt.ylabel('Precision')\n",
    "        plt.title(plot_title)\n",
    "        plt.legend(loc=\"best\", fontsize='small')\n",
    "        filename = os.path.join(save_dir, f\"{model_name}_precision_recall_curve.png\")\n",
    "        plt.savefig(filename, bbox_inches='tight')\n",
    "        print(f\"Saved Precision-Recall curve plot to {filename}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting Precision-Recall curve for {model_name}: {e}\")\n",
    "    finally:\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def plot_score_distribution(y_scores, classes, model_name, score_type=\"probability\", save_dir=PLOT_DIR):\n",
    "    plot_title = f'Prediction Score Distributions ({score_type.replace(\"_\",\" \").title()}) - {model_name}'\n",
    "    xlabel = score_type.replace(\"_\",\" \").title()\n",
    "\n",
    "    print(f\"Generating score distribution plot for {model_name} using {score_type} scores...\")\n",
    "    try:\n",
    "        if len(y_scores.shape) == 1:\n",
    "            if len(classes) == 2:\n",
    "                print(\"Plotting distribution for binary scores (assuming score for class 1).\")\n",
    "                n_classes_plot = 1\n",
    "                plot_labels = [f\"Score ({classes[1]})\"]\n",
    "                scores_to_plot = [y_scores]\n",
    "            else:\n",
    "                print(f\"Warning: Received 1D scores for {len(classes)} classes. Cannot plot distributions.\")\n",
    "                return\n",
    "        elif len(y_scores.shape) > 1:\n",
    "             n_classes_plot = y_scores.shape[1]\n",
    "             if n_classes_plot != len(classes):\n",
    "                 print(f\"Warning: Number of score columns ({n_classes_plot}) differs from classes ({len(classes)}). Using column index for labels.\")\n",
    "                 plot_labels = [f\"Score Col {i}\" for i in range(n_classes_plot)]\n",
    "             else:\n",
    "                 plot_labels = [f\"Score ({c})\" for c in classes]\n",
    "             scores_to_plot = [y_scores[:, i] for i in range(n_classes_plot)]\n",
    "        else:\n",
    "             print(f\"Unexpected shape for y_scores: {y_scores.shape}. Cannot plot.\")\n",
    "             return\n",
    "\n",
    "\n",
    "        if n_classes_plot == 0:\n",
    "            print(f\"No scores provided for {model_name}. Skipping distribution plot.\")\n",
    "            return\n",
    "        if n_classes_plot > 15:\n",
    "            print(f\"Skipping score distribution plot for {model_name} due to high number of classes/columns ({n_classes_plot}).\")\n",
    "            return\n",
    "\n",
    "        n_rows = (n_classes_plot + 1) // 2\n",
    "        fig, axes = plt.subplots(n_rows, 2, figsize=(15, 5 * n_rows), squeeze=False)\n",
    "        axes_flat = axes.flatten()\n",
    "\n",
    "        for i in range(n_classes_plot):\n",
    "            if i < len(axes_flat):\n",
    "                sns.histplot(scores_to_plot[i], bins=30, kde=True, ax=axes_flat[i])\n",
    "                axes_flat[i].set_title(f'{plot_labels[i]} Distribution')\n",
    "                axes_flat[i].set_xlabel(xlabel)\n",
    "                axes_flat[i].set_ylabel('Frequency')\n",
    "                if score_type == 'probability':\n",
    "                    axes_flat[i].set_xlim(0, 1)\n",
    "\n",
    "        for j in range(n_classes_plot, len(axes_flat)):\n",
    "            fig.delaxes(axes_flat[j])\n",
    "\n",
    "        plt.suptitle(plot_title, y=1.03)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        filename = os.path.join(save_dir, f\"{model_name}_score_distribution.png\")\n",
    "        plt.savefig(filename, bbox_inches='tight')\n",
    "        print(f\"Saved score distribution plot to {filename}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting score distribution for {model_name}: {e}\")\n",
    "    finally:\n",
    "        plt.close()\n",
    "\n",
    "def save_misclassified_examples(y_true, y_pred, texts, model_name, save_dir=PLOT_DIR, n_examples=50):\n",
    "    try:\n",
    "        y_true = np.asarray(y_true)\n",
    "        y_pred = np.asarray(y_pred)\n",
    "\n",
    "        misclassified_mask = y_true != y_pred\n",
    "        misclassified_indices = np.where(misclassified_mask)[0]\n",
    "\n",
    "        if len(misclassified_indices) == 0:\n",
    "            print(f\"No misclassifications found for {model_name}.\")\n",
    "            return\n",
    "\n",
    "        if isinstance(texts, (pd.Series, pd.DataFrame)):\n",
    "             try:\n",
    "                 if isinstance(texts.index, pd.RangeIndex) and texts.index.start == 0 and texts.index.step == 1:\n",
    "                     misclassified_texts = texts.iloc[misclassified_indices].tolist()\n",
    "                 else:\n",
    "                     original_indices = texts.index[misclassified_indices]\n",
    "                     misclassified_texts = texts.loc[original_indices].tolist()\n",
    "             except Exception as e:\n",
    "                 print(f\"Error extracting texts using index, falling back to iloc: {e}\")\n",
    "                 misclassified_texts = texts.iloc[misclassified_indices].tolist()\n",
    "        elif isinstance(texts, (list, np.ndarray)):\n",
    "             misclassified_texts = [texts[i] for i in misclassified_indices]\n",
    "        else:\n",
    "            print(f\"Unsupported text data type ({type(texts)}) for misclassified examples. Skipping.\")\n",
    "            return\n",
    "\n",
    "\n",
    "        actual_labels = y_true[misclassified_indices]\n",
    "        predicted_labels = y_pred[misclassified_indices]\n",
    "\n",
    "        misclassified_df = pd.DataFrame({\n",
    "            'Actual Label': actual_labels,\n",
    "            'Predicted Label': predicted_labels,\n",
    "            'Text': misclassified_texts\n",
    "        })\n",
    "\n",
    "        if len(misclassified_df) > n_examples:\n",
    "            misclassified_df = misclassified_df.sample(n=n_examples, random_state=42)\n",
    "\n",
    "        filename = os.path.join(save_dir, f\"{model_name}_misclassified_examples.csv\")\n",
    "        misclassified_df.to_csv(filename, index=False, encoding='utf-8')\n",
    "        print(f\"Saved {len(misclassified_df)} misclassified examples to {filename}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving misclassified examples for {model_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sM1BIlwdGJCl"
   },
   "outputs": [],
   "source": [
    "print(\"\\nGenerating plots for initial kNN...\")\n",
    "try:\n",
    "    if hasattr(knn, 'predict_proba'):\n",
    "        knn_probs = knn.predict_proba(X_val)\n",
    "        plot_multiclass_roc_curve(y_val, knn_probs, class_labels, \"kNN_Initial\")\n",
    "        plot_multiclass_precision_recall(y_val, knn_probs, class_labels, \"kNN_Initial\")\n",
    "        plot_score_distribution(knn_probs, class_labels, \"kNN_Initial\")\n",
    "    else:\n",
    "        print(\"kNN model does not support predict_proba. Skipping ROC, PR, Prob Dist plots.\")\n",
    "\n",
    "    plot_confusion_matrix(y_val, knn_preds, class_labels, \"kNN_Initial\")\n",
    "    save_misclassified_examples(y_val, knn_preds, val_df['text'], \"kNN_Initial\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during kNN_Initial plotting: {e}\")\n",
    "finally:\n",
    "    plt.close('all')\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qSCcEK1mGLUO"
   },
   "outputs": [],
   "source": [
    "print(\"\\nGenerating plots for initial Logistic Regression...\")\n",
    "try:\n",
    "    logreg_probs = logreg.predict_proba(X_val)\n",
    "    plot_multiclass_roc_curve(y_val, logreg_probs, class_labels, \"LogReg_Initial\")\n",
    "    plot_multiclass_precision_recall(y_val, logreg_probs, class_labels, \"LogReg_Initial\")\n",
    "    plot_score_distribution(logreg_probs, class_labels, \"LogReg_Initial\")\n",
    "    plot_confusion_matrix(y_val, logreg_preds, class_labels, \"LogReg_Initial\")\n",
    "    save_misclassified_examples(y_val, logreg_preds, val_df['text'], \"LogReg_Initial\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during LogReg_Initial plotting: {e}\")\n",
    "finally:\n",
    "    plt.close('all')\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZckSnsPgGOQ8"
   },
   "outputs": [],
   "source": [
    "print(\"\\nGenerating plots for initial SVM...\")\n",
    "y_scores = None\n",
    "score_type = None\n",
    "try:\n",
    "    if hasattr(svm_clf, 'predict_proba') and callable(getattr(svm_clf, 'predict_proba', None)) and getattr(svm_clf, 'probability', False):\n",
    "        y_scores = svm_clf.predict_proba(X_val)\n",
    "        score_type = \"probability\"\n",
    "        print(\"Using predict_proba for SVM plots.\")\n",
    "    elif hasattr(svm_clf, 'decision_function') and callable(getattr(svm_clf, 'decision_function', None)):\n",
    "        y_scores = svm_clf.decision_function(X_val)\n",
    "        score_type = \"decision_function\"\n",
    "        print(\"Using decision_function for SVM plots.\")\n",
    "    else:\n",
    "        print(\"SVM model supports neither predict_proba nor decision_function. Skipping score-based plots.\")\n",
    "\n",
    "    plot_confusion_matrix(y_val, svm_preds, class_labels, \"SVM_Initial\")\n",
    "    save_misclassified_examples(y_val, svm_preds, val_df['text'], \"SVM_Initial\")\n",
    "\n",
    "    if y_scores is not None and score_type is not None:\n",
    "        plot_multiclass_roc_curve(y_val, y_scores, class_labels, \"SVM_Initial\", score_type=score_type)\n",
    "        plot_multiclass_precision_recall(y_val, y_scores, class_labels, \"SVM_Initial\", score_type=score_type)\n",
    "        plot_score_distribution(y_scores, class_labels, \"SVM_Initial\", score_type=score_type)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during SVM_Initial plotting: {e}\")\n",
    "finally:\n",
    "    plt.close('all')\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xI_h2C5xGSQY"
   },
   "outputs": [],
   "source": [
    "print(\"\\nGenerating plots for Tuned kNN...\")\n",
    "try:\n",
    "    best_knn = random_search_knn.best_estimator_\n",
    "    if hasattr(best_knn, 'predict_proba'):\n",
    "        knn_tuned_probs = best_knn.predict_proba(X_val)\n",
    "        plot_multiclass_roc_curve(y_val, knn_tuned_probs, class_labels, \"kNN_Tuned\")\n",
    "        plot_multiclass_precision_recall(y_val, knn_tuned_probs, class_labels, \"kNN_Tuned\")\n",
    "        plot_score_distribution(knn_tuned_probs, class_labels, \"kNN_Tuned\")\n",
    "    else:\n",
    "         print(\"Tuned kNN model does not support predict_proba. Skipping ROC, PR, Prob Dist plots.\")\n",
    "\n",
    "    plot_confusion_matrix(y_val, knn_tuned_preds, class_labels, \"kNN_Tuned\")\n",
    "    save_misclassified_examples(y_val, knn_tuned_preds, val_df['text'], \"kNN_Tuned\")\n",
    "\n",
    "except NameError:\n",
    "     print(\"Skipping Tuned kNN plots - 'random_search_knn' or 'knn_tuned_preds' not defined.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during kNN_Tuned plotting: {e}\")\n",
    "finally:\n",
    "    plt.close('all')\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ga19DLBsGUnW"
   },
   "outputs": [],
   "source": [
    "print(\"\\nGenerating plots for Tuned LogReg...\")\n",
    "try:\n",
    "    best_logreg = random_search_logreg.best_estimator_\n",
    "    logreg_tuned_probs = best_logreg.predict_proba(X_val)\n",
    "    plot_multiclass_roc_curve(y_val, logreg_tuned_probs, class_labels, \"LogReg_Tuned\")\n",
    "    plot_multiclass_precision_recall(y_val, logreg_tuned_probs, class_labels, \"LogReg_Tuned\")\n",
    "    plot_score_distribution(logreg_tuned_probs, class_labels, \"LogReg_Tuned\")\n",
    "    plot_confusion_matrix(y_val, logreg_tuned_preds, class_labels, \"LogReg_Tuned\")\n",
    "    save_misclassified_examples(y_val, logreg_tuned_preds, val_df['text'], \"LogReg_Tuned\")\n",
    "\n",
    "except NameError:\n",
    "     print(\"Skipping Tuned LogReg plots - 'random_search_logreg' or 'logreg_tuned_preds' not defined.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during LogReg_Tuned plotting: {e}\")\n",
    "finally:\n",
    "    plt.close('all')\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nu29jQJyGWwE"
   },
   "outputs": [],
   "source": [
    "print(\"\\nGenerating plots for Tuned SVM...\")\n",
    "y_scores_tuned = None\n",
    "score_type_tuned = None\n",
    "try:\n",
    "    best_svm = random_search_svm.best_estimator_\n",
    "\n",
    "    if hasattr(best_svm, 'predict_proba') and callable(getattr(best_svm, 'predict_proba', None)) and getattr(best_svm, 'probability', False):\n",
    "         y_scores_tuned = best_svm.predict_proba(X_val)\n",
    "         score_type_tuned = \"probability\"\n",
    "         print(\"Using predict_proba for Tuned SVM plots.\")\n",
    "    elif hasattr(best_svm, 'decision_function') and callable(getattr(best_svm, 'decision_function', None)):\n",
    "        y_scores_tuned = best_svm.decision_function(X_val)\n",
    "        score_type_tuned = \"decision_function\"\n",
    "        print(\"Using decision_function for Tuned SVM plots.\")\n",
    "    else:\n",
    "        print(\"Tuned SVM model supports neither predict_proba nor decision_function. Skipping score-based plots.\")\n",
    "\n",
    "    plot_confusion_matrix(y_val, svm_tuned_preds, class_labels, \"SVM_Tuned\")\n",
    "    save_misclassified_examples(y_val, svm_tuned_preds, val_df['text'], \"SVM_Tuned\")\n",
    "\n",
    "    if y_scores_tuned is not None and score_type_tuned is not None:\n",
    "        plot_multiclass_roc_curve(y_val, y_scores_tuned, class_labels, \"SVM_Tuned\", score_type=score_type_tuned)\n",
    "        plot_multiclass_precision_recall(y_val, y_scores_tuned, class_labels, \"SVM_Tuned\", score_type=score_type_tuned)\n",
    "        plot_score_distribution(y_scores_tuned, class_labels, \"SVM_Tuned\", score_type=score_type_tuned)\n",
    "\n",
    "except NameError:\n",
    "     print(\"Skipping Tuned SVM plots - 'random_search_svm' or 'svm_tuned_preds' not defined.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during SVM_Tuned plotting: {e}\")\n",
    "finally:\n",
    "    plt.close('all')\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8WAb4z6wGg18"
   },
   "source": [
    "### Test Labels Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tdBaJMhcGyW2"
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- Analyzing the Generated Predictions File ---\")\n",
    "prediction_file_path = output_file_path\n",
    "prediction_column = 'label'\n",
    "\n",
    "if not os.path.exists(prediction_file_path):\n",
    "    print(f\"Error: Prediction file not found at '{prediction_file_path}'.\")\n",
    "    print(\"Please ensure the previous script block ran successfully and created the file.\")\n",
    "else:\n",
    "    print(f\"Loading prediction file: {prediction_file_path}\")\n",
    "    try:\n",
    "        predictions_df = pd.read_csv(prediction_file_path)\n",
    "        print(f\"Successfully loaded {len(predictions_df)} rows.\")\n",
    "\n",
    "        print(\"\\n--- Basic File Information ---\")\n",
    "        print(f\"Columns in the file: {predictions_df.columns.tolist()}\")\n",
    "        print(f\"Shape of the dataframe: {predictions_df.shape}\")\n",
    "\n",
    "        if prediction_column not in predictions_df.columns:\n",
    "            print(f\"\\nError: The specified prediction column '{prediction_column}' was not found in the file.\")\n",
    "            print(\"Please check the column name used when saving the predictions.\")\n",
    "        else:\n",
    "            print(f\"\\n--- Analyzing Predictions ('{prediction_column}' column) ---\")\n",
    "\n",
    "            missing_predictions = predictions_df[prediction_column].isnull().sum()\n",
    "            total_predictions = len(predictions_df)\n",
    "            print(f\"Total predictions generated: {total_predictions}\")\n",
    "            if missing_predictions > 0:\n",
    "                print(f\"Warning: Found {missing_predictions} missing predictions ({missing_predictions / total_predictions * 100:.2f}%).\")\n",
    "            else:\n",
    "                print(\"No missing predictions found in the prediction column.\")\n",
    "\n",
    "            print(\"\\nPredicted Class Distribution:\")\n",
    "            class_counts = predictions_df[prediction_column].value_counts().sort_index()\n",
    "            class_percentages = predictions_df[prediction_column].value_counts(normalize=True).sort_index() * 100\n",
    "\n",
    "            distribution_summary = pd.DataFrame({\n",
    "                'Count': class_counts,\n",
    "                'Percentage': class_percentages\n",
    "            })\n",
    "            distribution_summary['Percentage'] = distribution_summary['Percentage'].map('{:.2f}%'.format)\n",
    "\n",
    "            print(distribution_summary)\n",
    "\n",
    "            unique_classes = predictions_df[prediction_column].nunique()\n",
    "            print(f\"\\nNumber of unique predicted classes: {unique_classes}\")\n",
    "            print(f\"Predicted classes: {sorted(predictions_df[prediction_column].unique().tolist())}\")\n",
    "\n",
    "            try:\n",
    "                if total_predictions > 0 and total_predictions < 500000:\n",
    "                    print(\"\\nGenerating plot for predicted class distribution...\")\n",
    "                    plt.figure(figsize=(12, 6))\n",
    "                    sns.countplot(data=predictions_df, x=prediction_column, order=class_counts.index)\n",
    "                    plt.title('Distribution of Predicted Classes')\n",
    "                    plt.xlabel('Predicted Class Label')\n",
    "                    plt.ylabel('Frequency Count')\n",
    "                    plt.xticks(rotation=45, ha='right')\n",
    "                    plt.tight_layout()\n",
    "                    plot_filename = os.path.join(PLOT_DIR, \"predicted_class_distribution.png\")\n",
    "                    plt.savefig(plot_filename, bbox_inches='tight')\n",
    "                    print(f\"Saved predicted class distribution plot to: {plot_filename}\")\n",
    "                    plt.close()\n",
    "                else:\n",
    "                    print(\"\\nSkipping plot generation due to large dataset size or no predictions.\")\n",
    "            except Exception as plot_err:\n",
    "                print(f\"\\nCould not generate plot: {plot_err}\")\n",
    "\n",
    "        del predictions_df\n",
    "        gc.collect()\n",
    "        print(\"\\nCleaned up predictions DataFrame from memory.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred while reading or processing the file '{prediction_file_path}': {e}\")\n",
    "\n",
    "print(\"\\n--- Analysis script finished ---\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "zND-CyGqCXpA",
    "ZuBEEIaHCvc_",
    "z66zsv1su8xf",
    "FDUwO98nEFDx",
    "6IyFa3DbEJwF",
    "NEiaNjl-EYTN",
    "42pPYmjMEfE2",
    "qjplXZwOElUq",
    "k707XgRJFAqo",
    "BWimwBlhFHF7",
    "BCoJywdXfVTe",
    "2kCBepCVeaf_",
    "_pSb3JYLFPGs",
    "qYJpB9lwFSh5",
    "8WAb4z6wGg18"
   ],
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
